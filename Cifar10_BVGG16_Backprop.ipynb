{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to implement binarized version of VGG16 on Cifar-10 dataset, using the traditional backpropagation method and the simulation approach for forward-mode autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "DEVICE = torch.device('cuda')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset,device=None):\n",
    "    final_X, final_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        final_X.append(x)\n",
    "        final_Y.append(y)\n",
    "    X = torch.stack(final_X)\n",
    "    Y = torch.tensor(final_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return TensorDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Cifar10_dl(batch_size_train=256, batch_size_eval=1024, device=DEVICE):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    data_train = CIFAR10('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [45000,5000])\n",
    "    \n",
    "    data_test = CIFAR10('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dl(batch_size_train=1024, batch_size_eval=1024, device=torch.device('cuda')):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = MNIST('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [55000,5000])\n",
    "    \n",
    "    data_test = MNIST('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1].cpu().detach().numpy() for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, max(loss))\n",
    "  ax2.set_ylim(0.0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "  acc = []\n",
    "  for X, y in dl:\n",
    "    #acc.append((torch.sigmoid(model(X)) > 0.5) == y)\n",
    "    acc.append(torch.argmax(model(X), dim=1) == y)\n",
    "  acc = torch.cat(acc)\n",
    "  acc = torch.sum(acc)/len(acc)\n",
    "  return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binarize(x, quant_mode = 'det'):\n",
    "    if quant_mode == 'det':\n",
    "        return x.sign()\n",
    "    else:\n",
    "        return x.add_(1).div_(2).add_(torch.rand(x.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input.data=Binarize(input.data)\n",
    "\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = nn.functional.linear(input, self.weight)\n",
    "\n",
    "        if not hasattr(self.bias,'org'):\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "        self.bias.data = Binarize(self.bias.org)\n",
    "        out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarizeConv2D(nn.Conv2d):\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeConv2D, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input, ba = True):\n",
    "        if ba:\n",
    "            input.data=Binarize(input.data)\n",
    "        \n",
    "        if not hasattr(self.weight, 'org'):\n",
    "            self.weight.org = self.weight.data.clone()\n",
    "        self.weight.data = Binarize(self.weight.org)\n",
    "\n",
    "        if not hasattr(self.bias, 'org'):\n",
    "            self.bias.org = self.bias.clone()\n",
    "        self.bias.data = Binarize(self.bias.org)\n",
    "\n",
    "        out = torch.nn.functional.conv2d(input, self.weight, self.bias, stride = self.stride, padding = self.padding)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BVGG16, self).__init__()\n",
    "        self.conv1 = BinarizeConv2D(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.htanh1 = nn.Hardtanh()\n",
    "        #self.htanh1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = BinarizeConv2D(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.htanh2 = nn.Hardtanh()\n",
    "        #self.htanh2 = nn.ReLU()\n",
    "        self.maxpooling2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = BinarizeConv2D(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.htanh3 = nn.Hardtanh()\n",
    "        #self.htanh3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = BinarizeConv2D(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.htanh4 = nn.Hardtanh()\n",
    "        #self.htanh4 = nn.ReLU()\n",
    "        self.maxpooling4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = BinarizeConv2D(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        #self.htanh5 = nn.ReLU()\n",
    "        self.htanh5 = nn.Hardtanh()\n",
    "\n",
    "        self.conv6 = BinarizeConv2D(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        #self.htanh6 = nn.ReLU()\n",
    "        self.htanh6 = nn.Hardtanh()\n",
    "\n",
    "        self.conv7 = BinarizeConv2D(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh7 = nn.Hardtanh()\n",
    "        self.maxpooling7 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        '''\n",
    "        self.conv8 = BinarizeConv2D(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh8 = nn.Hardtanh()\n",
    "\n",
    "        self.conv9 = BinarizeConv2D(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn9 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh9 = nn.Hardtanh()\n",
    "\n",
    "        self.conv10 = BinarizeConv2D(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn10 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh10 = nn.Hardtanh()\n",
    "        self.maxpooling10 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv11 = BinarizeConv2D(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh11 = nn.Hardtanh()\n",
    "\n",
    "        self.conv12 = BinarizeConv2D(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh12 = nn.Hardtanh()\n",
    "\n",
    "        self.conv13 = BinarizeConv2D(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn13 = nn.BatchNorm2d(512)\n",
    "        #self.htanh7 = nn.ReLU()\n",
    "        self.htanh13 = nn.Hardtanh()\n",
    "        self.maxpooling13 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        '''\n",
    "        self.fc14 = BinarizeLinear(4*4*512, 1024)\n",
    "        self.bn14 = nn.BatchNorm1d(1024)\n",
    "        self.htanh14 = nn.Hardtanh()\n",
    "\n",
    "        self.fc15 = BinarizeLinear(1024, 1024)\n",
    "        self.bn15 = nn.BatchNorm1d(1024)\n",
    "        self.htanh15=  nn.Hardtanh()\n",
    "\n",
    "        self.fc16 = BinarizeLinear(1024, 10)\n",
    "        #self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.htanh1(self.bn1(self.conv1(input, ba = False)))\n",
    "        x = self.maxpooling2(self.htanh2(self.bn2(self.conv2(x))))\n",
    "        x = self.htanh3(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpooling4(self.htanh4(self.bn4(self.conv4(x))))\n",
    "        x = self.htanh5(self.bn5(self.conv5(x)))\n",
    "        x = self.htanh6(self.bn6(self.conv6(x)))\n",
    "        x = self.maxpooling7(self.htanh7(self.bn7(self.conv7(x))))\n",
    "        '''\n",
    "        x = self.htanh8(self.bn8(self.conv8(x)))\n",
    "        x = self.htanh9(self.bn9(self.conv9(x)))\n",
    "        x = self.maxpooling10(self.htanh10(self.bn10(self.conv10(x))))\n",
    "        x = self.htanh11(self.bn11(self.conv11(x)))\n",
    "        x = self.htanh12(self.bn12(self.conv12(x)))\n",
    "        x = self.maxpooling13(self.htanh13(self.bn13(self.conv13(x))))\n",
    "        '''\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.htanh14(self.bn14(self.fc14(x)))\n",
    "        x = self.htanh15(self.bn15(self.fc15(x)))\n",
    "        x = self.fc16(x)\n",
    "        #x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BMLP_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BMLP_1, self).__init__()\n",
    "        self.fc_1 = BinarizeLinear(28*28, 1024, bias = True, device = DEVICE)\n",
    "        self.htan_1 = nn.Hardtanh()\n",
    "        self.bn_1 = nn.BatchNorm1d(1024, device = DEVICE)\n",
    "\n",
    "        self.fc_2 = BinarizeLinear(1024, 1024, bias = True, device = DEVICE)\n",
    "        self.htan_2 = nn.Hardtanh()\n",
    "        self.bn_2 = nn.BatchNorm1d(1024, device = DEVICE)\n",
    "\n",
    "        self.fc_3 = BinarizeLinear(1024, 1024, bias = True, device = DEVICE)\n",
    "        self.htan_3 = nn.Hardtanh()\n",
    "        self.bn_3 = nn.BatchNorm1d(1024, device = DEVICE)\n",
    "        \n",
    "        self.fc_4 = BinarizeLinear(1024, 10, bias = True, device = DEVICE)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.reshape(input, (input.shape[0], -1))\n",
    "        x = self.fc_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.htan_1(x)\n",
    "        #x = self.bn_1(x)\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.htan_2(x)\n",
    "        #x = self.bn_2(x)\n",
    "\n",
    "        x = self.fc_3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = self.htan_3(x)\n",
    "        #x = self.bn_3(x)\n",
    "        \n",
    "        x = self.fc_4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, opt, scheduler, softweight_flag, trick_flag, bits_storage, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_dir):\n",
    "    itr = -1\n",
    "    stats = {'train-loss' : [], 'valid-acc' : []}\n",
    "    model.train()\n",
    "    accumulation = {}\n",
    "    change = {}\n",
    "    threshold = bits_storage\n",
    "\n",
    "    if use_forward_grad:\n",
    "        random_dir = {}\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            random_dir[i] = 0\n",
    "\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        accumulation[i] = torch.zeros(p.data.shape, device = DEVICE)\n",
    "        change[i] = torch.zeros(p.data.shape, device = DEVICE)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        for x, y in train_dl:\n",
    "            itr += 1\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            stats['train-loss'].append((itr, loss))\n",
    "            loss.backward()\n",
    "\n",
    "            if use_forward_grad:\n",
    "                with torch.no_grad():\n",
    "                    da = torch.zeros((num_dir, 1), device = DEVICE)\n",
    "\n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        g = p.grad.view(-1)\n",
    "                        v = torch.randn(num_dir, len(g), device = DEVICE)\n",
    "                        da += (v@g).view(num_dir, 1)\n",
    "                        random_dir[i] = v\n",
    "                    \n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        g = torch.mean(da * random_dir[i], dim = 0)\n",
    "                        p.grad = g.view(p.grad.shape)\n",
    "\n",
    "\n",
    "            if softweight_flag:\n",
    "                for p in list(model.parameters()):\n",
    "                    if hasattr(p, 'org'):\n",
    "                        p.data.copy_(p.org)\n",
    "            elif trick_flag:\n",
    "                for i, p in enumerate(model.parameters()):\n",
    "                    if hasattr(p, 'org'):\n",
    "                        #p.grad = p.grad*10\n",
    "                        tmp_accumulation = accumulation[i] + p.grad.sign()#.int()\n",
    "                        tmp_accumulation = tmp_accumulation.clamp_(-bits_storage, bits_storage)\n",
    "                        possible_pos = (tmp_accumulation.sign() == p.grad.sign())\n",
    "                        accumulation[i] = tmp_accumulation.clone()\n",
    "                        p.grad = p.grad*torch.abs(tmp_accumulation)*possible_pos\n",
    "                        p.grad[torch.abs(tmp_accumulation)>(bits_storage-0.5)]*=(1e12/(1/opt.state_dict()['param_groups'][0]['lr']))\n",
    "                        #p.grad[torch.abs(tmp_accumulation)>31.5] = p.data[torch.abs(tmp_accumulation)>31.5].sign()*100000\n",
    "                        change[i] = ((p.data.sign()*p.grad)>(1/opt.state_dict()['param_groups'][0]['lr']))\n",
    "                        accumulation[i] = accumulation[i] * ~change[i]\n",
    "            \n",
    "            opt.step()\n",
    "            if itr%2000 == 0 and itr != 0 and bits_storage<128:\n",
    "                bits_storage *= 2\n",
    "            #scheduler.step()\n",
    "\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p, 'org'):\n",
    "                    if softweight_flag:\n",
    "                        p.org.copy_(p.data.clamp_(-1,1))\n",
    "                    else:\n",
    "                        p.org.copy_(p.data.sign())\n",
    "            \n",
    "            if itr % 100 == 0:\n",
    "                valid_acc = get_acc(model, valid_dl)\n",
    "                stats['valid-acc'].append((itr, valid_acc))\n",
    "                s = f\"{epoch}:{itr} [train] loss:{loss.item():.3f}, [valid] acc:{valid_acc:.3f}\"\n",
    "                print(s)\n",
    "        #scheduler.step()\n",
    "            \n",
    "    test_acc = get_acc(model, test_dl)\n",
    "    print(f\"[test] acc:{test_acc:.3f}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11781962\n"
     ]
    }
   ],
   "source": [
    "model = BVGG16().to(DEVICE)\n",
    "print(count_parameters(model))\n",
    "\n",
    "train_batch_size = 128\n",
    "test_batch_size = 1024\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
    "#opt = torch.optim.SGD(model.parameters(), lr = 1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.98)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1500, gamma=0.2)\n",
    "\n",
    "softweight_flag = True\n",
    "trick_flag = False\n",
    "bits_storage = 16\n",
    "\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "#criterion = nn.MultiMarginLoss(p = 2)\n",
    "\n",
    "max_epochs = 500\n",
    "\n",
    "use_forward_grad = True\n",
    "num_dir = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0:0 [train] loss:48.982, [valid] acc:0.104\n",
      "0:100 [train] loss:52.259, [valid] acc:0.093\n",
      "0:200 [train] loss:46.334, [valid] acc:0.107\n",
      "0:300 [train] loss:45.851, [valid] acc:0.097\n",
      "1:400 [train] loss:55.076, [valid] acc:0.100\n",
      "1:500 [train] loss:44.620, [valid] acc:0.104\n",
      "1:600 [train] loss:46.636, [valid] acc:0.098\n",
      "1:700 [train] loss:48.984, [valid] acc:0.093\n",
      "2:800 [train] loss:47.566, [valid] acc:0.094\n",
      "2:900 [train] loss:53.127, [valid] acc:0.107\n",
      "2:1000 [train] loss:51.077, [valid] acc:0.092\n",
      "3:1100 [train] loss:47.480, [valid] acc:0.107\n",
      "3:1200 [train] loss:46.908, [valid] acc:0.101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_dl, valid_dl, test_dl \u001b[39m=\u001b[39m get_Cifar10_dl(train_batch_size, test_batch_size, device \u001b[39m=\u001b[39m DEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#train_dl, valid_dl, test_dl = get_mnist_dl(train_batch_size, test_batch_size, device = DEVICE)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m stats \u001b[39m=\u001b[39m run_experiment(model, opt, scheduler, softweight_flag, trick_flag, bits_storage, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_dir)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m print_stats(stats)\n",
      "\u001b[1;32m/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb Cell 16\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, opt, scheduler, softweight_flag, trick_flag, bits_storage, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_dir)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     change[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(p\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mshape, device \u001b[39m=\u001b[39m DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_dl:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         itr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703139222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-bnn/Cifar10_BVGG16_Backprop.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataset.py:188\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataset.py:188\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = get_Cifar10_dl(train_batch_size, test_batch_size, device = DEVICE)\n",
    "#train_dl, valid_dl, test_dl = get_mnist_dl(train_batch_size, test_batch_size, device = DEVICE)\n",
    "stats = run_experiment(model, opt, scheduler, softweight_flag, trick_flag, bits_storage, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_dir)\n",
    "print_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('BNN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb840f806f29f410adf72128552a18fefb24267895bb11ac579fa6a12231d74f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
