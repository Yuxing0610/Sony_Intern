{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import FashionMNIST, MNIST, CIFAR10, SVHN\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vision_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 7, 3, 8, 1, 4])\n",
      "tensor([4, 0, 2, 5, 1, 3])\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([2, 5, 3, 6, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2,7,3,8,1,4])\n",
    "print(a)\n",
    "sort_a_index = torch.argsort(a)\n",
    "print(sort_a_index)\n",
    "random = torch.tensor([1,2,3,4,5,6])\n",
    "random = torch.sort(random).values\n",
    "print(random)\n",
    "tmp = torch.zeros(len(random)).long()\n",
    "tmp[sort_a_index] = torch.arange(len(random))\n",
    "random = random[tmp]\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset,device=None):\n",
    "    final_X, final_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        final_X.append(x)\n",
    "        final_Y.append(y)\n",
    "    X = torch.stack(final_X)\n",
    "    Y = torch.tensor(final_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return TensorDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Cifar10_dl(batch_size_train=256, batch_size_eval=1024, device=DEVICE):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = CIFAR10('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [45000,5000])\n",
    "    \n",
    "    data_test = CIFAR10('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dl(batch_size_train=1024, batch_size_eval=1024, device=torch.device('cuda')):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = MNIST('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [55000,5000])\n",
    "    \n",
    "    data_test = MNIST('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "  model.eval()\n",
    "  acc = []\n",
    "  for X, y in dl:\n",
    "    #acc.append((torch.sigmoid(model(X)) > 0.5) == y)\n",
    "    acc.append(torch.argmax(model(X), dim=1) == y)\n",
    "  acc = torch.cat(acc)\n",
    "  acc = torch.sum(acc)/len(acc)\n",
    "  model.train()\n",
    "  return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats = {'train-loss' : [], 'valid-acc' : [], 'diff_mean' : [], 'diff_variance' : [], 'cosine' : [], 'scale_diff' : []}\n",
    "def print_stats_full(stats):\n",
    "\n",
    "  fig, axs= plt.subplots(3, 2, figsize=(7,9), dpi=110)\n",
    "\n",
    "  axs[0, 0].set_title(\"ERM loss\")\n",
    "  axs[0, 1].set_title(\"Valid Acc\")\n",
    "  axs[1, 0].set_title(\"diff mean\")\n",
    "  axs[1, 1].set_title(\"diff variance\")\n",
    "  axs[2, 0].set_title(\"cosine\")\n",
    "  axs[2, 1].set_title(\"scale_diff\")\n",
    "  #axs[2, 1].set_title(\"same_sign_perc\")\n",
    "\n",
    "\n",
    "  for i in range(3):\n",
    "    for j in range(2):\n",
    "      axs[i, j].set_xlabel(\"iterations\")\n",
    "      axs[i, j].grid()\n",
    "  \n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1] for x in stats['train-loss']]\n",
    "  axs[0, 0].set_ylim(0.0, max(loss))\n",
    "  axs[0, 0].plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  axs[0, 1].set_ylim(0.0, 1.05)\n",
    "  axs[0, 1].plot(itrs, acc)\n",
    "\n",
    "  itrs = [x[0] for x in stats['diff_mean']]\n",
    "  diff_mean = [x[1].cpu() for x in stats['diff_mean']]\n",
    "  axs[1, 0].set_ylim(min(diff_mean), max(diff_mean))\n",
    "  axs[1, 0].plot(itrs, diff_mean)\n",
    "\n",
    "  itrs = [x[0] for x in stats['diff_mean']]\n",
    "  diff_variance = [x[1].cpu() for x in stats['diff_mean']]\n",
    "  axs[1, 1].set_ylim(0.0, max(diff_variance))\n",
    "  axs[1, 1].plot(itrs, diff_variance)\n",
    "\n",
    "  itrs = [x[0] for x in stats['cosine']]\n",
    "  cosine = [x[1].cpu() for x in stats['cosine']]\n",
    "  axs[2, 0].set_ylim(min(cosine), max(cosine))\n",
    "  axs[2, 0].plot(itrs, cosine)\n",
    "\n",
    "  \n",
    "  itrs = [x[0] for x in stats['scale_diff']]\n",
    "  scale_diff = [x[1].cpu() for x in stats['scale_diff']]\n",
    "  axs[2, 1].set_ylim(min(scale_diff), max(scale_diff))\n",
    "  axs[2, 1].plot(itrs, scale_diff)\n",
    "  '''\n",
    "  itrs = [x[0] for x in stats['same_sign_perc']]\n",
    "  scale_diff = [x[1].cpu() for x in stats['same_sign_perc']]\n",
    "  axs[2, 1].set_ylim(0, 1)\n",
    "  axs[2, 1].plot(itrs, scale_diff)\n",
    "  '''\n",
    "  plt.tight_layout()\n",
    "  fig.savefig('testing.jpg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1] for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, max(loss))\n",
    "  ax2.set_ylim(0.0, 1.05)\n",
    "  fig.savefig('testing.jpg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU())\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        '''\n",
    "        self.layer9 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU())\n",
    "        self.layer13 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        '''\n",
    "        self.fc = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(4*4*512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU())\n",
    "        self.fc3= nn.Sequential(\n",
    "            nn.Linear(1024, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out)\n",
    "        #out = self.layer9(out)\n",
    "        '''\n",
    "        out = self.layer10(out)\n",
    "        out = self.layer11(out)\n",
    "        out = self.layer12(out)\n",
    "        out = self.layer13(out)\n",
    "        '''\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardtanh())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardtanh(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Hardtanh())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Hardtanh(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(7*7*128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Hardtanh())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Hardtanh())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Hardtanh())\n",
    "        self.fc4= nn.Sequential(\n",
    "            nn.Linear(1024, num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, opt, scheduler, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_dir, use_momentum, check_gap, shuffle_ratio, decay_rate):\n",
    "    itr = -1\n",
    "    stats = {'train-loss' : [], 'valid-acc' : [], 'diff_mean' : [], 'diff_variance' : [], 'cosine' : [], 'scale_diff' : [], 'same_sign_perc' : []}\n",
    "\n",
    "    if use_forward_grad:\n",
    "        random_dir = {}\n",
    "        if use_momentum:\n",
    "            master_dir = {}\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                random_dir[i] = 0\n",
    "                master_dir[i] = 0\n",
    "        else:\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                random_dir[i] = 0\n",
    "        \n",
    "    for epoch in range(max_epochs):\n",
    "        for x, y in train_dl:\n",
    "            itr += 1\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "\n",
    "            if use_forward_grad:\n",
    "                with torch.no_grad():\n",
    "                    da = torch.zeros((num_dir, 1), device = DEVICE)\n",
    "                \n",
    "                    if use_momentum:\n",
    "                        if itr % check_gap == 0:\n",
    "                            for i, p in enumerate(model.parameters()):\n",
    "                                g = p.grad.view(-1)\n",
    "                                index_sort = torch.argsort(g)\n",
    "                                master_dir[i] = index_sort\n",
    "                    \n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        g = p.grad.view(-1)\n",
    "                        v = torch.randn(num_dir, len(g), device = DEVICE)\n",
    "\n",
    "                        if use_momentum:\n",
    "                            for j in range(num_dir):\n",
    "                                v[j] = torch.sort(v[j]).values\n",
    "                                tmp = torch.zeros(len(v[j])).to(DEVICE).long()\n",
    "                                tmp_dir = torch.tensor([]).to(DEVICE).long()\n",
    "\n",
    "                                if shuffle_ratio != 0:\n",
    "                                    for k in range(int(1/shuffle_ratio)):\n",
    "                                        if k<(int(1/shuffle_ratio)-1):\n",
    "                                            partial_master_dir = master_dir[i][int(k*shuffle_ratio*master_dir[i].nelement()) : int((k+1)*shuffle_ratio*master_dir[i].nelement())]\n",
    "                                        else:\n",
    "                                            partial_master_dir = master_dir[i][int(k*shuffle_ratio*master_dir[i].nelement()) : ]\n",
    "                                        partial_shuffle_idx = torch.randperm(partial_master_dir.nelement())\n",
    "                                        partial_tmp_dir = partial_master_dir[partial_shuffle_idx]\n",
    "                                        tmp_dir = torch.cat((tmp_dir, partial_tmp_dir))\n",
    "\n",
    "                                else:\n",
    "                                    tmp_dir = master_dir[i]\n",
    "                                \n",
    "                                tmp[tmp_dir] = torch.arange(len(v[j])).to(DEVICE)\n",
    "                                v[j] = v[j][tmp]\n",
    "                            \n",
    "                            v_random = torch.randn(num_dir, len(g), device = DEVICE)\n",
    "                            v = v_random*(min(0.9, 1-(decay_rate**(itr%check_gap)))) + v*(max(0.1, decay_rate**(itr%check_gap)))\n",
    "\n",
    "                        random_dir[i] = v\n",
    "                        da += (v@g).view(num_dir, 1)\n",
    "\n",
    "                    estimation = torch.tensor([]).to(DEVICE)\n",
    "                    true_gradient = torch.tensor([]).to(DEVICE)\n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        g = da*random_dir[i]\n",
    "                        true_gradient = torch.cat((true_gradient, p.grad.view(-1)), dim = 0)\n",
    "                        estimation = torch.cat((estimation, torch.mean(g, dim = 0)), dim = 0)\n",
    "                        p.grad = torch.mean(g, dim = 0).view(p.grad.shape)\n",
    "                \n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            stats['train-loss'].append((itr, loss.item()))\n",
    "\n",
    "            if use_forward_grad:\n",
    "                same_sign_perc = torch.sum(true_gradient.sign() == estimation.sign())/torch.numel(true_gradient)\n",
    "                scale_true_gradient = true_gradient.norm()\n",
    "                scale_estimation = estimation.norm()\n",
    "                stats['diff_mean'].append((itr, torch.mean(estimation - true_gradient)))\n",
    "                stats['diff_variance'].append((itr, torch.var(estimation - true_gradient)))\n",
    "                stats['cosine'].append((itr, torch.sum(true_gradient*estimation)/(scale_true_gradient*scale_estimation)))\n",
    "                stats['scale_diff'].append((itr, scale_estimation - scale_true_gradient))\n",
    "                stats['same_sign_perc'].append((itr, same_sign_perc))\n",
    "            \n",
    "            if itr % 50 == 0:\n",
    "                valid_acc = get_acc(model, valid_dl)\n",
    "                stats['valid-acc'].append((itr, valid_acc))\n",
    "                s = f\"{epoch}:{itr} [train] loss:{loss.item():.3f}, [valid] acc:{valid_acc:.3f}\"\n",
    "                print(s)\n",
    "    \n",
    "    test_acc = get_acc(model, test_dl)\n",
    "    print(f\"[test] acc:{test_acc:.3f}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8798922\n"
     ]
    }
   ],
   "source": [
    "#model= VGG16().to(DEVICE)\n",
    "model = CNN().to(DEVICE)\n",
    "print(count_parameters(model))\n",
    "\n",
    "train_batch_size = 5\n",
    "test_batch_size = 1024\n",
    "\n",
    "train_dl, valid_dl, test_dl = get_mnist_dl(train_batch_size, test_batch_size, device = DEVICE)\n",
    "#train_dl, valid_dl, test_dl = get_Cifar10_dl(train_batch_size, test_batch_size, device = DEVICE)\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr = 1e-1)\n",
    "#opt = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=1000, gamma=0.8)\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "max_epochs = 2\n",
    "\n",
    "use_forward_grad = False\n",
    "num_dir = 1\n",
    "use_momentum = False\n",
    "check_gap = 100\n",
    "shuffle_ratio = 0.2\n",
    "decay_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 [train] loss:2.286, [valid] acc:0.110\n",
      "0:50 [train] loss:5.765, [valid] acc:0.283\n",
      "0:100 [train] loss:4.802, [valid] acc:0.335\n",
      "0:150 [train] loss:4.247, [valid] acc:0.445\n",
      "0:200 [train] loss:0.000, [valid] acc:0.549\n",
      "0:250 [train] loss:10.204, [valid] acc:0.464\n",
      "0:300 [train] loss:5.859, [valid] acc:0.392\n",
      "0:350 [train] loss:4.213, [valid] acc:0.563\n",
      "0:400 [train] loss:9.405, [valid] acc:0.571\n",
      "0:450 [train] loss:0.146, [valid] acc:0.575\n",
      "0:500 [train] loss:2.012, [valid] acc:0.731\n",
      "0:550 [train] loss:2.668, [valid] acc:0.604\n",
      "0:600 [train] loss:4.107, [valid] acc:0.712\n",
      "0:650 [train] loss:2.514, [valid] acc:0.657\n",
      "0:700 [train] loss:0.836, [valid] acc:0.779\n",
      "0:750 [train] loss:1.723, [valid] acc:0.758\n",
      "0:800 [train] loss:0.000, [valid] acc:0.750\n",
      "0:850 [train] loss:3.641, [valid] acc:0.830\n",
      "0:900 [train] loss:3.036, [valid] acc:0.746\n",
      "0:950 [train] loss:6.069, [valid] acc:0.700\n",
      "0:1000 [train] loss:2.727, [valid] acc:0.775\n",
      "0:1050 [train] loss:5.677, [valid] acc:0.843\n",
      "0:1100 [train] loss:4.132, [valid] acc:0.827\n",
      "0:1150 [train] loss:1.102, [valid] acc:0.783\n",
      "0:1200 [train] loss:0.013, [valid] acc:0.865\n",
      "0:1250 [train] loss:2.010, [valid] acc:0.833\n",
      "0:1300 [train] loss:0.003, [valid] acc:0.835\n",
      "0:1350 [train] loss:4.773, [valid] acc:0.847\n",
      "0:1400 [train] loss:0.124, [valid] acc:0.763\n",
      "0:1450 [train] loss:0.045, [valid] acc:0.770\n",
      "0:1500 [train] loss:1.562, [valid] acc:0.892\n",
      "0:1550 [train] loss:0.002, [valid] acc:0.842\n",
      "0:1600 [train] loss:3.472, [valid] acc:0.904\n",
      "0:1650 [train] loss:0.001, [valid] acc:0.856\n",
      "0:1700 [train] loss:1.045, [valid] acc:0.859\n",
      "0:1750 [train] loss:1.528, [valid] acc:0.878\n",
      "0:1800 [train] loss:1.310, [valid] acc:0.883\n",
      "0:1850 [train] loss:2.085, [valid] acc:0.906\n",
      "0:1900 [train] loss:0.149, [valid] acc:0.915\n",
      "0:1950 [train] loss:0.816, [valid] acc:0.906\n",
      "0:2000 [train] loss:1.318, [valid] acc:0.923\n",
      "0:2050 [train] loss:1.131, [valid] acc:0.913\n",
      "0:2100 [train] loss:0.001, [valid] acc:0.905\n",
      "0:2150 [train] loss:0.011, [valid] acc:0.907\n",
      "0:2200 [train] loss:0.018, [valid] acc:0.926\n",
      "0:2250 [train] loss:1.819, [valid] acc:0.885\n",
      "0:2300 [train] loss:0.001, [valid] acc:0.871\n",
      "0:2350 [train] loss:1.187, [valid] acc:0.887\n",
      "0:2400 [train] loss:0.142, [valid] acc:0.923\n",
      "0:2450 [train] loss:2.479, [valid] acc:0.926\n",
      "0:2500 [train] loss:0.023, [valid] acc:0.888\n",
      "0:2550 [train] loss:0.000, [valid] acc:0.938\n",
      "0:2600 [train] loss:1.491, [valid] acc:0.915\n",
      "0:2650 [train] loss:0.059, [valid] acc:0.926\n",
      "0:2700 [train] loss:1.476, [valid] acc:0.914\n",
      "0:2750 [train] loss:0.043, [valid] acc:0.901\n",
      "0:2800 [train] loss:0.013, [valid] acc:0.940\n",
      "0:2850 [train] loss:0.023, [valid] acc:0.911\n",
      "0:2900 [train] loss:0.000, [valid] acc:0.922\n",
      "0:2950 [train] loss:0.148, [valid] acc:0.935\n",
      "0:3000 [train] loss:0.047, [valid] acc:0.938\n",
      "0:3050 [train] loss:0.055, [valid] acc:0.941\n",
      "0:3100 [train] loss:1.965, [valid] acc:0.939\n",
      "0:3150 [train] loss:0.013, [valid] acc:0.919\n",
      "0:3200 [train] loss:0.047, [valid] acc:0.936\n",
      "0:3250 [train] loss:0.041, [valid] acc:0.909\n",
      "0:3300 [train] loss:0.216, [valid] acc:0.940\n",
      "0:3350 [train] loss:1.339, [valid] acc:0.917\n",
      "0:3400 [train] loss:0.809, [valid] acc:0.851\n",
      "0:3450 [train] loss:0.001, [valid] acc:0.941\n",
      "0:3500 [train] loss:0.042, [valid] acc:0.947\n",
      "0:3550 [train] loss:0.006, [valid] acc:0.939\n",
      "0:3600 [train] loss:2.713, [valid] acc:0.938\n",
      "0:3650 [train] loss:0.085, [valid] acc:0.942\n",
      "0:3700 [train] loss:0.937, [valid] acc:0.942\n",
      "0:3750 [train] loss:2.181, [valid] acc:0.945\n",
      "0:3800 [train] loss:0.607, [valid] acc:0.939\n",
      "0:3850 [train] loss:0.016, [valid] acc:0.931\n",
      "0:3900 [train] loss:0.425, [valid] acc:0.943\n",
      "0:3950 [train] loss:0.014, [valid] acc:0.940\n",
      "0:4000 [train] loss:0.013, [valid] acc:0.912\n",
      "0:4050 [train] loss:0.247, [valid] acc:0.935\n",
      "0:4100 [train] loss:0.166, [valid] acc:0.944\n",
      "0:4150 [train] loss:0.000, [valid] acc:0.953\n",
      "0:4200 [train] loss:0.258, [valid] acc:0.943\n",
      "0:4250 [train] loss:0.017, [valid] acc:0.960\n",
      "0:4300 [train] loss:0.063, [valid] acc:0.955\n",
      "0:4350 [train] loss:0.081, [valid] acc:0.947\n",
      "0:4400 [train] loss:0.518, [valid] acc:0.955\n",
      "0:4450 [train] loss:1.519, [valid] acc:0.954\n",
      "0:4500 [train] loss:0.134, [valid] acc:0.955\n",
      "0:4550 [train] loss:0.000, [valid] acc:0.954\n",
      "0:4600 [train] loss:0.049, [valid] acc:0.945\n",
      "0:4650 [train] loss:0.210, [valid] acc:0.949\n",
      "0:4700 [train] loss:0.373, [valid] acc:0.947\n",
      "0:4750 [train] loss:0.975, [valid] acc:0.923\n",
      "0:4800 [train] loss:0.040, [valid] acc:0.959\n",
      "0:4850 [train] loss:0.004, [valid] acc:0.952\n",
      "0:4900 [train] loss:0.001, [valid] acc:0.960\n",
      "0:4950 [train] loss:0.002, [valid] acc:0.953\n",
      "0:5000 [train] loss:0.023, [valid] acc:0.948\n",
      "0:5050 [train] loss:0.119, [valid] acc:0.939\n",
      "0:5100 [train] loss:1.459, [valid] acc:0.953\n",
      "0:5150 [train] loss:0.001, [valid] acc:0.960\n",
      "0:5200 [train] loss:0.017, [valid] acc:0.963\n",
      "0:5250 [train] loss:0.003, [valid] acc:0.967\n",
      "0:5300 [train] loss:0.462, [valid] acc:0.964\n",
      "0:5350 [train] loss:0.044, [valid] acc:0.965\n",
      "0:5400 [train] loss:0.003, [valid] acc:0.948\n",
      "0:5450 [train] loss:0.030, [valid] acc:0.964\n",
      "0:5500 [train] loss:0.061, [valid] acc:0.961\n",
      "0:5550 [train] loss:0.060, [valid] acc:0.956\n",
      "0:5600 [train] loss:0.230, [valid] acc:0.949\n",
      "0:5650 [train] loss:0.029, [valid] acc:0.960\n",
      "0:5700 [train] loss:0.001, [valid] acc:0.962\n",
      "0:5750 [train] loss:0.178, [valid] acc:0.962\n",
      "0:5800 [train] loss:0.000, [valid] acc:0.966\n",
      "0:5850 [train] loss:1.652, [valid] acc:0.940\n",
      "0:5900 [train] loss:0.000, [valid] acc:0.962\n",
      "0:5950 [train] loss:0.042, [valid] acc:0.965\n",
      "0:6000 [train] loss:0.011, [valid] acc:0.963\n",
      "0:6050 [train] loss:0.350, [valid] acc:0.961\n",
      "0:6100 [train] loss:0.110, [valid] acc:0.959\n",
      "0:6150 [train] loss:1.198, [valid] acc:0.958\n",
      "0:6200 [train] loss:0.034, [valid] acc:0.950\n",
      "0:6250 [train] loss:0.078, [valid] acc:0.962\n",
      "0:6300 [train] loss:0.406, [valid] acc:0.965\n",
      "0:6350 [train] loss:0.079, [valid] acc:0.966\n",
      "0:6400 [train] loss:0.263, [valid] acc:0.962\n",
      "0:6450 [train] loss:0.002, [valid] acc:0.966\n",
      "0:6500 [train] loss:0.005, [valid] acc:0.962\n",
      "0:6550 [train] loss:0.088, [valid] acc:0.963\n",
      "0:6600 [train] loss:0.005, [valid] acc:0.968\n",
      "0:6650 [train] loss:0.215, [valid] acc:0.964\n",
      "0:6700 [train] loss:0.643, [valid] acc:0.964\n",
      "0:6750 [train] loss:0.007, [valid] acc:0.965\n",
      "0:6800 [train] loss:0.029, [valid] acc:0.963\n",
      "0:6850 [train] loss:0.666, [valid] acc:0.956\n",
      "0:6900 [train] loss:0.133, [valid] acc:0.962\n",
      "0:6950 [train] loss:1.051, [valid] acc:0.960\n",
      "0:7000 [train] loss:0.106, [valid] acc:0.965\n",
      "0:7050 [train] loss:0.033, [valid] acc:0.961\n",
      "0:7100 [train] loss:0.003, [valid] acc:0.967\n",
      "0:7150 [train] loss:0.003, [valid] acc:0.964\n",
      "0:7200 [train] loss:0.007, [valid] acc:0.964\n",
      "0:7250 [train] loss:1.294, [valid] acc:0.967\n",
      "0:7300 [train] loss:0.095, [valid] acc:0.970\n",
      "0:7350 [train] loss:0.101, [valid] acc:0.969\n",
      "0:7400 [train] loss:0.001, [valid] acc:0.968\n",
      "0:7450 [train] loss:0.001, [valid] acc:0.968\n",
      "0:7500 [train] loss:0.077, [valid] acc:0.969\n",
      "0:7550 [train] loss:0.113, [valid] acc:0.965\n",
      "0:7600 [train] loss:0.253, [valid] acc:0.970\n",
      "0:7650 [train] loss:0.003, [valid] acc:0.972\n",
      "0:7700 [train] loss:0.004, [valid] acc:0.967\n",
      "0:7750 [train] loss:1.506, [valid] acc:0.967\n",
      "0:7800 [train] loss:0.013, [valid] acc:0.968\n",
      "0:7850 [train] loss:0.059, [valid] acc:0.970\n",
      "0:7900 [train] loss:0.004, [valid] acc:0.964\n",
      "0:7950 [train] loss:0.003, [valid] acc:0.970\n",
      "0:8000 [train] loss:0.018, [valid] acc:0.967\n",
      "0:8050 [train] loss:0.002, [valid] acc:0.967\n",
      "0:8100 [train] loss:0.001, [valid] acc:0.973\n",
      "0:8150 [train] loss:1.351, [valid] acc:0.969\n",
      "0:8200 [train] loss:0.288, [valid] acc:0.967\n",
      "0:8250 [train] loss:0.025, [valid] acc:0.961\n",
      "0:8300 [train] loss:0.001, [valid] acc:0.964\n",
      "0:8350 [train] loss:0.007, [valid] acc:0.970\n",
      "0:8400 [train] loss:0.000, [valid] acc:0.970\n",
      "0:8450 [train] loss:0.013, [valid] acc:0.972\n",
      "0:8500 [train] loss:0.190, [valid] acc:0.967\n",
      "0:8550 [train] loss:0.165, [valid] acc:0.957\n",
      "0:8600 [train] loss:0.308, [valid] acc:0.968\n",
      "0:8650 [train] loss:0.016, [valid] acc:0.974\n",
      "0:8700 [train] loss:0.001, [valid] acc:0.970\n",
      "0:8750 [train] loss:0.073, [valid] acc:0.969\n",
      "0:8800 [train] loss:0.001, [valid] acc:0.968\n",
      "0:8850 [train] loss:1.207, [valid] acc:0.969\n",
      "0:8900 [train] loss:0.051, [valid] acc:0.969\n",
      "0:8950 [train] loss:0.012, [valid] acc:0.972\n",
      "0:9000 [train] loss:0.039, [valid] acc:0.968\n",
      "0:9050 [train] loss:0.423, [valid] acc:0.970\n",
      "0:9100 [train] loss:0.610, [valid] acc:0.970\n",
      "0:9150 [train] loss:0.018, [valid] acc:0.974\n",
      "0:9200 [train] loss:0.710, [valid] acc:0.972\n",
      "0:9250 [train] loss:0.017, [valid] acc:0.976\n",
      "0:9300 [train] loss:0.443, [valid] acc:0.973\n",
      "0:9350 [train] loss:0.106, [valid] acc:0.973\n",
      "0:9400 [train] loss:0.001, [valid] acc:0.974\n",
      "0:9450 [train] loss:0.007, [valid] acc:0.976\n",
      "0:9500 [train] loss:0.051, [valid] acc:0.972\n",
      "0:9550 [train] loss:0.079, [valid] acc:0.971\n",
      "0:9600 [train] loss:0.042, [valid] acc:0.973\n",
      "0:9650 [train] loss:0.029, [valid] acc:0.973\n",
      "0:9700 [train] loss:0.160, [valid] acc:0.973\n",
      "0:9750 [train] loss:0.212, [valid] acc:0.976\n",
      "0:9800 [train] loss:0.021, [valid] acc:0.973\n",
      "0:9850 [train] loss:0.178, [valid] acc:0.972\n",
      "0:9900 [train] loss:0.003, [valid] acc:0.973\n",
      "0:9950 [train] loss:0.192, [valid] acc:0.973\n",
      "0:10000 [train] loss:0.003, [valid] acc:0.971\n",
      "0:10050 [train] loss:0.046, [valid] acc:0.974\n",
      "0:10100 [train] loss:0.014, [valid] acc:0.975\n",
      "0:10150 [train] loss:0.067, [valid] acc:0.973\n",
      "0:10200 [train] loss:0.018, [valid] acc:0.970\n",
      "0:10250 [train] loss:0.013, [valid] acc:0.972\n",
      "0:10300 [train] loss:0.073, [valid] acc:0.973\n",
      "0:10350 [train] loss:0.003, [valid] acc:0.975\n",
      "0:10400 [train] loss:0.004, [valid] acc:0.971\n",
      "0:10450 [train] loss:0.029, [valid] acc:0.972\n",
      "0:10500 [train] loss:0.021, [valid] acc:0.972\n",
      "0:10550 [train] loss:0.243, [valid] acc:0.974\n",
      "0:10600 [train] loss:1.389, [valid] acc:0.974\n",
      "0:10650 [train] loss:0.157, [valid] acc:0.973\n",
      "0:10700 [train] loss:0.004, [valid] acc:0.974\n",
      "0:10750 [train] loss:0.231, [valid] acc:0.973\n",
      "0:10800 [train] loss:0.206, [valid] acc:0.975\n",
      "0:10850 [train] loss:0.040, [valid] acc:0.978\n",
      "0:10900 [train] loss:0.602, [valid] acc:0.973\n",
      "0:10950 [train] loss:0.001, [valid] acc:0.973\n",
      "1:11000 [train] loss:0.006, [valid] acc:0.968\n",
      "1:11050 [train] loss:0.017, [valid] acc:0.972\n",
      "1:11100 [train] loss:0.008, [valid] acc:0.974\n",
      "1:11150 [train] loss:0.056, [valid] acc:0.969\n",
      "1:11200 [train] loss:2.677, [valid] acc:0.973\n",
      "1:11250 [train] loss:1.311, [valid] acc:0.976\n",
      "1:11300 [train] loss:0.756, [valid] acc:0.975\n",
      "1:11350 [train] loss:0.176, [valid] acc:0.976\n",
      "1:11400 [train] loss:0.008, [valid] acc:0.976\n",
      "1:11450 [train] loss:0.251, [valid] acc:0.976\n",
      "1:11500 [train] loss:0.269, [valid] acc:0.977\n",
      "1:11550 [train] loss:0.019, [valid] acc:0.977\n",
      "1:11600 [train] loss:0.011, [valid] acc:0.976\n",
      "1:11650 [train] loss:0.003, [valid] acc:0.977\n",
      "1:11700 [train] loss:0.261, [valid] acc:0.976\n",
      "1:11750 [train] loss:0.165, [valid] acc:0.975\n",
      "1:11800 [train] loss:0.227, [valid] acc:0.976\n",
      "1:11850 [train] loss:0.018, [valid] acc:0.974\n",
      "1:11900 [train] loss:0.006, [valid] acc:0.974\n",
      "1:11950 [train] loss:0.018, [valid] acc:0.968\n",
      "1:12000 [train] loss:0.001, [valid] acc:0.975\n",
      "1:12050 [train] loss:0.127, [valid] acc:0.973\n",
      "1:12100 [train] loss:0.008, [valid] acc:0.975\n",
      "1:12150 [train] loss:0.044, [valid] acc:0.975\n",
      "1:12200 [train] loss:0.013, [valid] acc:0.974\n",
      "1:12250 [train] loss:0.004, [valid] acc:0.978\n",
      "1:12300 [train] loss:0.001, [valid] acc:0.977\n",
      "1:12350 [train] loss:0.005, [valid] acc:0.980\n",
      "1:12400 [train] loss:0.615, [valid] acc:0.975\n",
      "1:12450 [train] loss:0.022, [valid] acc:0.976\n",
      "1:12500 [train] loss:0.012, [valid] acc:0.976\n",
      "1:12550 [train] loss:0.009, [valid] acc:0.973\n",
      "1:12600 [train] loss:0.258, [valid] acc:0.975\n",
      "1:12650 [train] loss:0.031, [valid] acc:0.977\n",
      "1:12700 [train] loss:0.166, [valid] acc:0.978\n",
      "1:12750 [train] loss:0.088, [valid] acc:0.977\n",
      "1:12800 [train] loss:0.033, [valid] acc:0.978\n",
      "1:12850 [train] loss:0.154, [valid] acc:0.976\n",
      "1:12900 [train] loss:0.002, [valid] acc:0.978\n",
      "1:12950 [train] loss:0.008, [valid] acc:0.978\n",
      "1:13000 [train] loss:0.075, [valid] acc:0.977\n",
      "1:13050 [train] loss:0.042, [valid] acc:0.976\n",
      "1:13100 [train] loss:0.034, [valid] acc:0.977\n",
      "1:13150 [train] loss:0.002, [valid] acc:0.976\n",
      "1:13200 [train] loss:0.010, [valid] acc:0.975\n",
      "1:13250 [train] loss:0.113, [valid] acc:0.976\n",
      "1:13300 [train] loss:0.001, [valid] acc:0.975\n",
      "1:13350 [train] loss:0.002, [valid] acc:0.976\n",
      "1:13400 [train] loss:0.033, [valid] acc:0.977\n",
      "1:13450 [train] loss:0.074, [valid] acc:0.975\n",
      "1:13500 [train] loss:0.007, [valid] acc:0.975\n",
      "1:13550 [train] loss:0.456, [valid] acc:0.975\n",
      "1:13600 [train] loss:0.051, [valid] acc:0.977\n",
      "1:13650 [train] loss:0.233, [valid] acc:0.976\n",
      "1:13700 [train] loss:0.530, [valid] acc:0.977\n",
      "1:13750 [train] loss:0.153, [valid] acc:0.979\n",
      "1:13800 [train] loss:0.639, [valid] acc:0.977\n",
      "1:13850 [train] loss:0.450, [valid] acc:0.978\n",
      "1:13900 [train] loss:0.057, [valid] acc:0.974\n",
      "1:13950 [train] loss:0.306, [valid] acc:0.975\n",
      "1:14000 [train] loss:0.056, [valid] acc:0.976\n",
      "1:14050 [train] loss:0.001, [valid] acc:0.976\n",
      "1:14100 [train] loss:0.068, [valid] acc:0.978\n",
      "1:14150 [train] loss:0.791, [valid] acc:0.977\n",
      "1:14200 [train] loss:0.015, [valid] acc:0.975\n",
      "1:14250 [train] loss:1.065, [valid] acc:0.978\n",
      "1:14300 [train] loss:0.040, [valid] acc:0.979\n",
      "1:14350 [train] loss:0.035, [valid] acc:0.979\n",
      "1:14400 [train] loss:0.175, [valid] acc:0.978\n",
      "1:14450 [train] loss:0.013, [valid] acc:0.980\n",
      "1:14500 [train] loss:0.004, [valid] acc:0.978\n",
      "1:14550 [train] loss:0.099, [valid] acc:0.977\n",
      "1:14600 [train] loss:0.005, [valid] acc:0.978\n",
      "1:14650 [train] loss:0.003, [valid] acc:0.979\n",
      "1:14700 [train] loss:0.001, [valid] acc:0.980\n",
      "1:14750 [train] loss:0.001, [valid] acc:0.979\n",
      "1:14800 [train] loss:0.002, [valid] acc:0.979\n",
      "1:14850 [train] loss:0.002, [valid] acc:0.978\n",
      "1:14900 [train] loss:0.333, [valid] acc:0.978\n",
      "1:14950 [train] loss:0.048, [valid] acc:0.977\n",
      "1:15000 [train] loss:0.001, [valid] acc:0.979\n",
      "1:15050 [train] loss:0.402, [valid] acc:0.979\n",
      "1:15100 [train] loss:0.870, [valid] acc:0.981\n",
      "1:15150 [train] loss:0.014, [valid] acc:0.977\n",
      "1:15200 [train] loss:0.078, [valid] acc:0.979\n",
      "1:15250 [train] loss:0.004, [valid] acc:0.978\n",
      "1:15300 [train] loss:0.502, [valid] acc:0.979\n",
      "1:15350 [train] loss:0.021, [valid] acc:0.979\n",
      "1:15400 [train] loss:0.487, [valid] acc:0.979\n",
      "1:15450 [train] loss:0.009, [valid] acc:0.978\n",
      "1:15500 [train] loss:1.170, [valid] acc:0.981\n",
      "1:15550 [train] loss:0.007, [valid] acc:0.980\n",
      "1:15600 [train] loss:0.007, [valid] acc:0.978\n",
      "1:15650 [train] loss:0.013, [valid] acc:0.980\n",
      "1:15700 [train] loss:0.270, [valid] acc:0.978\n",
      "1:15750 [train] loss:0.061, [valid] acc:0.978\n",
      "1:15800 [train] loss:0.607, [valid] acc:0.979\n",
      "1:15850 [train] loss:0.239, [valid] acc:0.977\n",
      "1:15900 [train] loss:0.007, [valid] acc:0.980\n",
      "1:15950 [train] loss:0.056, [valid] acc:0.979\n",
      "1:16000 [train] loss:0.082, [valid] acc:0.981\n",
      "1:16050 [train] loss:0.004, [valid] acc:0.980\n",
      "1:16100 [train] loss:0.018, [valid] acc:0.979\n",
      "1:16150 [train] loss:0.006, [valid] acc:0.981\n",
      "1:16200 [train] loss:0.001, [valid] acc:0.979\n",
      "1:16250 [train] loss:0.001, [valid] acc:0.979\n",
      "1:16300 [train] loss:0.867, [valid] acc:0.980\n",
      "1:16350 [train] loss:0.133, [valid] acc:0.979\n",
      "1:16400 [train] loss:0.012, [valid] acc:0.979\n",
      "1:16450 [train] loss:0.012, [valid] acc:0.981\n",
      "1:16500 [train] loss:0.010, [valid] acc:0.979\n",
      "1:16550 [train] loss:0.017, [valid] acc:0.976\n",
      "1:16600 [train] loss:0.056, [valid] acc:0.979\n",
      "1:16650 [train] loss:0.002, [valid] acc:0.979\n",
      "1:16700 [train] loss:0.139, [valid] acc:0.981\n",
      "1:16750 [train] loss:0.001, [valid] acc:0.980\n",
      "1:16800 [train] loss:0.003, [valid] acc:0.978\n",
      "1:16850 [train] loss:0.087, [valid] acc:0.980\n",
      "1:16900 [train] loss:0.132, [valid] acc:0.979\n",
      "1:16950 [train] loss:1.103, [valid] acc:0.979\n",
      "1:17000 [train] loss:0.007, [valid] acc:0.980\n",
      "1:17050 [train] loss:0.004, [valid] acc:0.980\n",
      "1:17100 [train] loss:0.003, [valid] acc:0.979\n",
      "1:17150 [train] loss:0.002, [valid] acc:0.978\n",
      "1:17200 [train] loss:0.024, [valid] acc:0.979\n",
      "1:17250 [train] loss:0.460, [valid] acc:0.980\n",
      "1:17300 [train] loss:0.004, [valid] acc:0.978\n",
      "1:17350 [train] loss:0.002, [valid] acc:0.978\n",
      "1:17400 [train] loss:0.548, [valid] acc:0.979\n",
      "1:17450 [train] loss:0.086, [valid] acc:0.978\n",
      "1:17500 [train] loss:0.507, [valid] acc:0.978\n",
      "1:17550 [train] loss:0.010, [valid] acc:0.978\n",
      "1:17600 [train] loss:0.073, [valid] acc:0.980\n",
      "1:17650 [train] loss:0.009, [valid] acc:0.979\n",
      "1:17700 [train] loss:0.721, [valid] acc:0.979\n",
      "1:17750 [train] loss:0.111, [valid] acc:0.979\n",
      "1:17800 [train] loss:0.191, [valid] acc:0.978\n",
      "1:17850 [train] loss:0.003, [valid] acc:0.979\n",
      "1:17900 [train] loss:0.004, [valid] acc:0.981\n",
      "1:17950 [train] loss:0.226, [valid] acc:0.980\n",
      "1:18000 [train] loss:0.093, [valid] acc:0.979\n",
      "1:18050 [train] loss:0.005, [valid] acc:0.981\n",
      "1:18100 [train] loss:0.005, [valid] acc:0.979\n",
      "1:18150 [train] loss:0.267, [valid] acc:0.978\n",
      "1:18200 [train] loss:0.003, [valid] acc:0.980\n",
      "1:18250 [train] loss:0.006, [valid] acc:0.979\n",
      "1:18300 [train] loss:0.014, [valid] acc:0.980\n",
      "1:18350 [train] loss:0.006, [valid] acc:0.979\n",
      "1:18400 [train] loss:0.244, [valid] acc:0.979\n",
      "1:18450 [train] loss:0.298, [valid] acc:0.980\n",
      "1:18500 [train] loss:0.005, [valid] acc:0.978\n",
      "1:18550 [train] loss:0.086, [valid] acc:0.978\n",
      "1:18600 [train] loss:0.001, [valid] acc:0.981\n",
      "1:18650 [train] loss:0.090, [valid] acc:0.979\n",
      "1:18700 [train] loss:0.032, [valid] acc:0.977\n",
      "1:18750 [train] loss:0.119, [valid] acc:0.981\n",
      "1:18800 [train] loss:0.004, [valid] acc:0.981\n",
      "1:18850 [train] loss:0.737, [valid] acc:0.979\n",
      "1:18900 [train] loss:0.076, [valid] acc:0.980\n",
      "1:18950 [train] loss:0.047, [valid] acc:0.979\n",
      "1:19000 [train] loss:0.005, [valid] acc:0.980\n",
      "1:19050 [train] loss:0.046, [valid] acc:0.979\n",
      "1:19100 [train] loss:0.478, [valid] acc:0.980\n",
      "1:19150 [train] loss:0.447, [valid] acc:0.980\n",
      "1:19200 [train] loss:0.011, [valid] acc:0.980\n",
      "1:19250 [train] loss:0.060, [valid] acc:0.981\n",
      "1:19300 [train] loss:0.002, [valid] acc:0.981\n",
      "1:19350 [train] loss:0.005, [valid] acc:0.981\n",
      "1:19400 [train] loss:0.136, [valid] acc:0.979\n",
      "1:19450 [train] loss:0.013, [valid] acc:0.980\n",
      "1:19500 [train] loss:0.285, [valid] acc:0.980\n",
      "1:19550 [train] loss:0.047, [valid] acc:0.980\n",
      "1:19600 [train] loss:0.162, [valid] acc:0.980\n",
      "1:19650 [train] loss:0.381, [valid] acc:0.980\n",
      "1:19700 [train] loss:0.002, [valid] acc:0.980\n",
      "1:19750 [train] loss:0.263, [valid] acc:0.977\n",
      "1:19800 [train] loss:0.213, [valid] acc:0.979\n",
      "1:19850 [train] loss:0.317, [valid] acc:0.982\n",
      "1:19900 [train] loss:0.731, [valid] acc:0.981\n",
      "1:19950 [train] loss:0.079, [valid] acc:0.979\n",
      "1:20000 [train] loss:0.271, [valid] acc:0.980\n",
      "1:20050 [train] loss:0.166, [valid] acc:0.978\n",
      "1:20100 [train] loss:0.046, [valid] acc:0.980\n",
      "1:20150 [train] loss:0.123, [valid] acc:0.978\n",
      "1:20200 [train] loss:0.011, [valid] acc:0.980\n",
      "1:20250 [train] loss:0.001, [valid] acc:0.980\n",
      "1:20300 [train] loss:0.018, [valid] acc:0.980\n",
      "1:20350 [train] loss:0.083, [valid] acc:0.979\n",
      "1:20400 [train] loss:0.017, [valid] acc:0.979\n",
      "1:20450 [train] loss:0.036, [valid] acc:0.979\n",
      "1:20500 [train] loss:0.001, [valid] acc:0.981\n",
      "1:20550 [train] loss:0.100, [valid] acc:0.978\n",
      "1:20600 [train] loss:0.304, [valid] acc:0.981\n",
      "1:20650 [train] loss:0.515, [valid] acc:0.981\n",
      "1:20700 [train] loss:0.033, [valid] acc:0.981\n",
      "1:20750 [train] loss:0.058, [valid] acc:0.981\n",
      "1:20800 [train] loss:0.006, [valid] acc:0.980\n",
      "1:20850 [train] loss:0.007, [valid] acc:0.981\n",
      "1:20900 [train] loss:0.043, [valid] acc:0.982\n",
      "1:20950 [train] loss:0.967, [valid] acc:0.980\n",
      "1:21000 [train] loss:0.003, [valid] acc:0.980\n",
      "1:21050 [train] loss:0.005, [valid] acc:0.979\n",
      "1:21100 [train] loss:0.004, [valid] acc:0.982\n",
      "1:21150 [train] loss:0.003, [valid] acc:0.980\n",
      "1:21200 [train] loss:0.008, [valid] acc:0.980\n",
      "1:21250 [train] loss:0.015, [valid] acc:0.979\n",
      "1:21300 [train] loss:0.466, [valid] acc:0.981\n",
      "1:21350 [train] loss:1.466, [valid] acc:0.981\n",
      "1:21400 [train] loss:0.018, [valid] acc:0.981\n",
      "1:21450 [train] loss:0.053, [valid] acc:0.981\n",
      "1:21500 [train] loss:0.009, [valid] acc:0.981\n",
      "1:21550 [train] loss:0.015, [valid] acc:0.980\n",
      "1:21600 [train] loss:0.115, [valid] acc:0.981\n",
      "1:21650 [train] loss:0.080, [valid] acc:0.979\n",
      "1:21700 [train] loss:0.060, [valid] acc:0.979\n",
      "1:21750 [train] loss:0.001, [valid] acc:0.981\n",
      "1:21800 [train] loss:0.018, [valid] acc:0.980\n",
      "1:21850 [train] loss:0.353, [valid] acc:0.981\n",
      "1:21900 [train] loss:2.098, [valid] acc:0.980\n",
      "1:21950 [train] loss:0.009, [valid] acc:0.978\n",
      "[test] acc:0.982\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 770x330 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = run_experiment(model, opt, scheduler, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_dir, use_momentum, check_gap, shuffle_ratio, decay_rate)\n",
    "if use_forward_grad:\n",
    "    print_stats_full(stats)\n",
    "else:\n",
    "    print_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb840f806f29f410adf72128552a18fefb24267895bb11ac579fa6a12231d74f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
