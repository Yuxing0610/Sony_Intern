{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook use the pytorch's build in API to implement forward-mode automatic differentation.\n",
    "\n",
    "Model: BVGG16\n",
    "\n",
    "Training method: Forward mode autodiff + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "DEVICE = torch.device('cuda')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset,device=None):\n",
    "    final_X, final_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        final_X.append(x)\n",
    "        final_Y.append(y)\n",
    "    X = torch.stack(final_X)\n",
    "    Y = torch.tensor(final_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return TensorDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Cifar10_dl(batch_size_train=256, batch_size_eval=1024, device=DEVICE):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    \n",
    "    data_train = CIFAR10('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [45000,5000])\n",
    "    \n",
    "    data_test = CIFAR10('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1].cpu().detach().numpy() for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, max(loss))\n",
    "  ax2.set_ylim(0.0, 1.05)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  fig.savefig('testing.jpg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acc(model, dl, device = DEVICE):\n",
    "    model.eval()\n",
    "    acc = []\n",
    "\n",
    "    for X, y in dl:\n",
    "        #one_hot_y = torch.zeros((X.shape[0], 10), device = DEVICE)\n",
    "        #one_hot_y[[i for i in range(X.shape[0])], [k.item() for k in y]] = 1\n",
    "        model.forward(X, y)\n",
    "        acc.append(torch.argmax(model.output, dim=1) == y)\n",
    "\n",
    "    model.train()\n",
    "    acc = torch.cat(acc)\n",
    "    acc = torch.sum(acc)/len(acc)\n",
    "\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里现在用的还是最简单的binarize的方法\n",
    "def Binarize(x, quant_mode = 'det'):\n",
    "    if quant_mode == 'det':\n",
    "        return x.sign()\n",
    "    else:\n",
    "        return x.add_(1).div_(2).add_(torch.rand(x.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_BinarizeLinear():\n",
    "    def __init__(self, input_dim, output_dim, num_dir, ba_flag, bda_flag, sw_flag, bits_storage = 0):\n",
    "        self.input_dim, self.output_dim = input_dim, output_dim\n",
    "        self.num_dir = num_dir\n",
    "        self.ba_flag = ba_flag\n",
    "        self.bda_flag = bda_flag\n",
    "        self.sw_flag = sw_flag\n",
    "        self.bits_storage = bits_storage\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim).to(DEVICE)\n",
    "\n",
    "        if sw_flag:\n",
    "            self.soft_w = self.linear.weight.data.clone()\n",
    "            self.soft_b = self.linear.bias.data.clone()\n",
    "        else:\n",
    "            self.soft_w = self.linear.weight.data.sign()\n",
    "            self.soft_b = self.linear.bias.data.sign()\n",
    "            self.accumulate_w = torch.zeros(self.linear.weight.data.shape, device = DEVICE)\n",
    "            self.change_w = torch.zeros(self.linear.weight.data.shape, device = DEVICE)\n",
    "            self.accumulate_b = torch.zeros(self.linear.bias.data.shape, device = DEVICE)\n",
    "            self.change_b = torch.zeros(self.linear.bias.data.shape, device = DEVICE)\n",
    "\n",
    "    def train_forward(self, input, da = None):\n",
    "        if self.ba_flag:\n",
    "            input.data = Binarize(input.data)\n",
    "\n",
    "        self.linear.weight.data = Binarize(self.soft_w)\n",
    "        self.vector_w = torch.randn((self.num_dir, self.output_dim, self.input_dim), device = DEVICE)\n",
    "        self.linear.bias.data = Binarize(self.soft_b)\n",
    "        self.vector_b = torch.randn((self.num_dir, self.output_dim), device = DEVICE)\n",
    "\n",
    "        if self.bda_flag:\n",
    "            self.vector_w = self.vector_w.sign()\n",
    "            self.vector_b = self.vector_b.sign()\n",
    "            if torch.is_tensor(da):\n",
    "                da = da.sign()\n",
    "        \n",
    "        new_da = torch.zeros((self.num_dir, input.shape[0], self.output_dim), device = DEVICE)\n",
    "\n",
    "        params = {name: p for name, p in self.linear.named_parameters()}\n",
    "\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(self.num_dir):\n",
    "                for name, p in params.items():\n",
    "                    delattr(self.linear, name)            \n",
    "                    if name == \"weight\":\n",
    "                        setattr(self.linear, name, fwAD.make_dual(p, self.vector_w[i]))\n",
    "                    elif name == \"bias\":\n",
    "                        setattr(self.linear, name, fwAD.make_dual(p, self.vector_b[i]))\n",
    "                \n",
    "                if torch.is_tensor(da):\n",
    "                    dual_input = fwAD.make_dual(input, da[i])\n",
    "                else:\n",
    "                    dual_input = input\n",
    "                \n",
    "                out = self.linear(dual_input)\n",
    "                new_da[i] = fwAD.unpack_dual(out).tangent\n",
    "\n",
    "        for name, p in params.items():\n",
    "            setattr(self.linear, name, p)\n",
    "\n",
    "        return out, new_da\n",
    "    \n",
    "    def eval_forward(self, input):\n",
    "        self.linear.weight.data = self.soft_w.sign()\n",
    "        self.linear.bias.data = self.soft_b.sign()\n",
    "        out = self.linear(input)\n",
    "        return out\n",
    "    \n",
    "    def update(self, da, lr):\n",
    "        gw = da.view(-1, 1, 1) * self.vector_w\n",
    "        gw = torch.mean(gw, dim = 0)\n",
    "        gb = da.view(-1, 1)*self.vector_b\n",
    "        gb = torch.mean(gb, dim = 0)\n",
    "        if self.sw_flag:\n",
    "            self.soft_w -= lr*gw\n",
    "            self.soft_b -= lr*gb\n",
    "\n",
    "            self.soft_w = self.soft_w.clamp_(-1, 1)\n",
    "            self.soft_b = self.soft_b.clamp_(-1, 1)\n",
    "        \n",
    "        else:\n",
    "            new_accumulate_w = self.accumulate_w + gw.sign()\n",
    "            new_accumulate_w = new_accumulate_w.clamp(-self.bits_storage, self.bits_storage)\n",
    "            self.accumulate_w = new_accumulate_w.clone()\n",
    "            possible_pos_w = (new_accumulate_w.sign() == gw.sign())\n",
    "            gw = gw * torch.abs(new_accumulate_w) * possible_pos_w\n",
    "            gw[torch.abs(new_accumulate_w) > (self.bits_storage - 0.5)] *= 1e10/lr\n",
    "\n",
    "            new_accumulate_b = self.accumulate_b + gb.sign()\n",
    "            new_accumulate_b = new_accumulate_b.clamp(-self.bits_storage, self.bits_storage)\n",
    "            self.accumulate_b = new_accumulate_b.clone()\n",
    "            possible_pos_b = (new_accumulate_b.sign() == gb.sign())\n",
    "            gb = gb * torch.abs(new_accumulate_b) * possible_pos_b\n",
    "            gb[torch.abs(new_accumulate_b) > (self.bits_storage - 0.5)] *= 1e10/lr\n",
    "\n",
    "            self.soft_w -= (lr*gw)\n",
    "            self.soft_w = self.soft_w.sign()\n",
    "            self.soft_b -= (lr*gb)\n",
    "            self.soft_b = self.soft_b.sign()\n",
    "\n",
    "            self.change_w = ((self.soft_w.sign() * gw) > 1/lr)\n",
    "            self.accumulate_w *= ~self.change_w\n",
    "            self.change_b = ((self.soft_b.sign() * gb) > 1/lr)\n",
    "            self.accumulate_b *= ~self.change_b      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_BinarizeConv2D():\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, num_dir, ba_flag, bda_flag, sw_flag, bits_storage = 0):\n",
    "        self.in_channel, self.out_channel = in_channel, out_channel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.num_dir = num_dir\n",
    "        self.ba_flag = ba_flag\n",
    "        self.bda_flag = bda_flag\n",
    "        self.sw_flag = sw_flag\n",
    "        self.bits_storage = bits_storage\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding).to(DEVICE)\n",
    "\n",
    "        if sw_flag:\n",
    "            self.soft_w = self.conv.weight.data.clone()\n",
    "            self.soft_b = self.conv.bias.data.clone()\n",
    "        else:\n",
    "            self.soft_w = self.conv.weight.data.sign()\n",
    "            self.soft_b = self.conv.bias.data.sign()\n",
    "            self.accumulate_w = torch.zeros(self.conv.weight.data.shape, device = DEVICE)\n",
    "            self.change_w = torch.zeros(self.conv.weight.data.shape, device = DEVICE)\n",
    "            self.accumulate_b = torch.zeros(self.conv.bias.data.shape, device = DEVICE)\n",
    "            self.change_b = torch.zeros(self.conv.bias.data.shape, device = DEVICE)\n",
    "        \n",
    "    def train_forward(self, input, da = None):\n",
    "        if self.ba_flag:\n",
    "            input.data = input.data.sign()\n",
    "        \n",
    "        self.conv.weight.data = self.soft_w.sign()\n",
    "        self.vector_w = torch.randn((self.num_dir, self.out_channel, self.in_channel, self.kernel_size, self.kernel_size), device = DEVICE)\n",
    "        self.conv.bias.data = self.soft_b.sign()\n",
    "        self.vector_b = torch.randn((self.num_dir, self.out_channel), device = DEVICE)\n",
    "\n",
    "        if self.bda_flag:\n",
    "            self.vector_w = self.vector_w.sign()\n",
    "            self.vector_b = self.vector_b.sign()\n",
    "            if torch.is_tensor(da):\n",
    "                da = da.sign()\n",
    "        \n",
    "        new_da = None\n",
    "\n",
    "        params = {name: p for name, p in self.conv.named_parameters()}\n",
    "\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(self.num_dir):\n",
    "                for name, p in params.items():\n",
    "                    delattr(self.conv, name)\n",
    "                    if name == \"weight\":\n",
    "                        setattr(self.conv, name, fwAD.make_dual(p, self.vector_w[i]))\n",
    "                    elif name== \"bias\":\n",
    "                        setattr(self.conv, name, fwAD.make_dual(p, self.vector_b[i]))\n",
    "\n",
    "                if torch.is_tensor(da):\n",
    "                    dual_input = fwAD.make_dual(input, da[i])\n",
    "                else:\n",
    "                    dual_input = input\n",
    "                out = self.conv(dual_input)\n",
    "                tmp_da = fwAD.unpack_dual(out).tangent.unsqueeze(0)\n",
    "                \n",
    "                if torch.is_tensor(new_da):\n",
    "                    new_da = torch.cat((new_da, tmp_da), dim = 0)\n",
    "                else:\n",
    "                    new_da = tmp_da\n",
    "            \n",
    "        for name, p in params.items():\n",
    "            setattr(self.conv, name, p)\n",
    "        \n",
    "        return out, new_da\n",
    "    \n",
    "    def eval_forward(self, input):\n",
    "        self.conv.weight.data = self.soft_w.sign()\n",
    "        self.conv.bias.data = self.soft_b.sign()\n",
    "        out = self.conv(input)\n",
    "        return out\n",
    "\n",
    "    def update(self, da, lr):\n",
    "        gw = da.view(-1, 1, 1, 1, 1) * self.vector_w\n",
    "        gw = torch.mean(gw, dim = 0)\n",
    "        gb = da.view(-1, 1)*self.vector_b\n",
    "        gb = torch.mean(gb, dim = 0)\n",
    "        if self.sw_flag:\n",
    "            self.soft_w -= lr*gw\n",
    "            self.soft_b -= lr*gb\n",
    "\n",
    "            self.soft_w = self.soft_w.clamp_(-1, 1)\n",
    "            self.soft_b = self.soft_b.clamp_(-1, 1)\n",
    "        \n",
    "        else:\n",
    "            new_accumulate_w = self.accumulate_w + gw.sign()\n",
    "            new_accumulate_w = new_accumulate_w.clamp(-self.bits_storage, self.bits_storage)\n",
    "            self.accumulate_w = new_accumulate_w.clone()\n",
    "            possible_pos_w = (new_accumulate_w.sign() == gw.sign())\n",
    "            gw = gw * torch.abs(new_accumulate_w) * possible_pos_w\n",
    "            gw[torch.abs(new_accumulate_w) > (self.bits_storage - 0.5)] *= 1e10/lr\n",
    "\n",
    "            new_accumulate_b = self.accumulate_b + gb.sign()\n",
    "            new_accumulate_b = new_accumulate_b.clamp(-self.bits_storage, self.bits_storage)\n",
    "            self.accumulate_b = new_accumulate_b.clone()\n",
    "            possible_pos_b = (new_accumulate_b.sign() == gb.sign())\n",
    "            gb = gb * torch.abs(new_accumulate_b) * possible_pos_b\n",
    "            gb[torch.abs(new_accumulate_b) > (self.bits_storage - 0.5)] *= 1e10/lr\n",
    "\n",
    "            self.soft_w -= (lr*gw)\n",
    "            self.soft_w = self.soft_w.sign()\n",
    "            self.soft_b -= (lr*gb)\n",
    "            self.soft_b = self.soft_b.sign()\n",
    "\n",
    "            self.change_w = ((self.soft_w.sign() * gw) > 1/lr)\n",
    "            self.accumulate_w *= ~self.change_w\n",
    "            self.change_b = ((self.soft_b.sign() * gb) > 1/lr)\n",
    "            self.accumulate_b *= ~self.change_b    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_Hardtanh():\n",
    "    def train_forward(self, input, da):\n",
    "        '''\n",
    "        input: batch_size * input_dim\n",
    "        da: num_dir * batch * input_dim\n",
    "        '''\n",
    "        num_dir = da.shape[0]\n",
    "        new_da = torch.zeros(da.shape, device = DEVICE)\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(num_dir):\n",
    "                dual_input = fwAD.make_dual(input, da[i])\n",
    "                out = nn.functional.hardtanh(dual_input)\n",
    "                new_da[i] = fwAD.unpack_dual(out).tangent\n",
    "        return out, new_da\n",
    "    \n",
    "    def eval_forward(self, input):\n",
    "        return nn.functional.hardtanh(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_MaxPool2d():\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride).to(DEVICE)\n",
    "    \n",
    "    def train_forward(self, input, da):\n",
    "        num_dir = da.shape[0]\n",
    "        new_da = None\n",
    "\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(num_dir):\n",
    "                dual_input = fwAD.make_dual(input, da[i])\n",
    "                out = self.maxpool(dual_input)\n",
    "                tmp_da = fwAD.unpack_dual(out).tangent.unsqueeze(0)\n",
    "                if torch.is_tensor(new_da):\n",
    "                    new_da = torch.cat((new_da, tmp_da), dim = 0)\n",
    "                else:\n",
    "                    new_da = tmp_da\n",
    "        return out, new_da\n",
    "    \n",
    "    def eval_forward(self, input):\n",
    "        return self.maxpool(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_Batchnorm1d():\n",
    "    def __init__(self, dim, bda_flag=False):\n",
    "        self.dim = dim\n",
    "        self.BN = nn.BatchNorm1d(self.dim).to(DEVICE)\n",
    "        self.bda_flag = bda_flag\n",
    "        self.BN.train()\n",
    "    \n",
    "    def train_forward(self, input, da):\n",
    "        num_dir = da.shape[0]\n",
    "        self.vector_w = torch.randn((num_dir, self.dim), device = DEVICE)#.sign()\n",
    "        self.vector_b = torch.randn((num_dir, self.dim), device = DEVICE)#.sign()\n",
    "\n",
    "        if self.bda_flag:\n",
    "            self.vector_w = self.vector_w.sign()\n",
    "            self.vector_b = self.vector_b.sign()\n",
    "\n",
    "        new_da = torch.zeros(da.shape, device = DEVICE)\n",
    "\n",
    "        params = {name: p for name, p in self.BN.named_parameters()}\n",
    "\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(num_dir):\n",
    "                for name, p in params.items():\n",
    "                    if name == 'weight':\n",
    "                        delattr(self.BN, name)\n",
    "                        setattr(self.BN, name, fwAD.make_dual(p, self.vector_w[i]))\n",
    "                    elif name == 'bias':\n",
    "                        delattr(self.BN, name)\n",
    "                        setattr(self.BN, name, fwAD.make_dual(p, self.vector_b[i]))\n",
    "                dual_input = fwAD.make_dual(input, da[i])\n",
    "\n",
    "                out = self.BN(dual_input)\n",
    "                new_da[i] = fwAD.unpack_dual(out).tangent\n",
    "\n",
    "        for name, p in params.items():\n",
    "            setattr(self.BN, name, p)\n",
    "        return out, da\n",
    "    \n",
    "    def eval_forward(self, input):\n",
    "        self.BN.eval()\n",
    "        out = self.BN(input)\n",
    "        self.BN.train()\n",
    "        return out\n",
    "    \n",
    "    def update(self, da, lr):\n",
    "        gw = da.view(-1, 1)*self.vector_w\n",
    "        gw = torch.mean(gw, dim = 0)\n",
    "        self.BN.weight.data -= lr*gw\n",
    "\n",
    "        gb = da.view(-1, 1)*self.vector_b\n",
    "        gb = torch.mean(gb, dim = 0)\n",
    "        self.BN.bias.data -= lr*gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_Batchnorm2d():\n",
    "    def __init__(self, dim, bda_flag=False):\n",
    "        self.dim = dim\n",
    "        self.BN = nn.BatchNorm2d(self.dim).to(DEVICE)\n",
    "        self.bda_flag = bda_flag\n",
    "        self.BN.train()\n",
    "\n",
    "    def train_forward(self, input, da):\n",
    "        num_dir = da.shape[0]\n",
    "        self.vector_w = torch.randn((num_dir, self.dim), device = DEVICE)\n",
    "        self.vector_b = torch.randn((num_dir, self.dim), device = DEVICE)\n",
    "\n",
    "        if self.bda_flag:\n",
    "            self.vector_w = self.vector_w.sign()\n",
    "            self.vector_b = self.vector_b.sign()\n",
    "        \n",
    "        new_da = torch.zeros(da.shape, device = DEVICE)\n",
    "\n",
    "        params = {name: p for name, p in self.BN.named_parameters()}\n",
    "\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(num_dir):\n",
    "                for name, p in params.items():\n",
    "                    delattr(self.BN, name)\n",
    "                    if name == \"weight\":\n",
    "                        setattr(self.BN, name, fwAD.make_dual(p, self.vector_w[i]))\n",
    "                    elif name == \"bias\":\n",
    "                        setattr(self.BN, name, fwAD.make_dual(p, self.vector_b[i]))\n",
    "                dual_input = fwAD.make_dual(input, da[i])\n",
    "\n",
    "                out = self.BN(dual_input)\n",
    "                new_da[i] = fwAD.unpack_dual(out).tangent\n",
    "        \n",
    "        for name, p in params.items():\n",
    "            setattr(self.BN, name, p)\n",
    "        return out, da\n",
    "    \n",
    "    def eval_forward(self, input):\n",
    "        self.BN.eval()\n",
    "        out = self.BN(input)\n",
    "        self.BN.train()\n",
    "        return out\n",
    "    \n",
    "    def update(self, da, lr):\n",
    "        gw = da.view(-1, 1)*self.vector_w\n",
    "        gw = torch.mean(gw, dim = 0)\n",
    "        self.BN.weight.data -= lr*gw\n",
    "\n",
    "        gb = da.view(-1, 1)*self.vector_b\n",
    "        gb = torch.mean(gb, dim = 0)\n",
    "        self.BN.bias.data -= lr*gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_Softmax_CrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.CE = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    \n",
    "    def train_forward(self, input, y, da):\n",
    "        num_dir = da.shape[0]\n",
    "        new_da = torch.zeros(num_dir, device = DEVICE)\n",
    "        with fwAD.dual_level():\n",
    "            for i in range(num_dir):\n",
    "                dual_input = fwAD.make_dual(input, da[i])\n",
    "                out = self.CE(dual_input, y)\n",
    "                new_da[i] = fwAD.unpack_dual(out).tangent\n",
    "        return out, new_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_BVGG(nn.Module):\n",
    "    def __init__(self, num_dir, bda_flag=False, sw_flag=True, bits_storage=0, device=DEVICE):\n",
    "        self.device = DEVICE\n",
    "        self.is_tarin = True\n",
    "\n",
    "        self.num_dir = num_dir\n",
    "        self.bda_flag = bda_flag\n",
    "        self.sw_flag = sw_flag\n",
    "        self.bits_storage = bits_storage\n",
    "\n",
    "        self.conv1 = self_BinarizeConv2D(3, 64, 3, 1, 1, num_dir=num_dir, ba_flag=False, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn1 = self_Batchnorm2d(64, bda_flag=bda_flag)\n",
    "        self.htanh1 = self_Hardtanh()\n",
    "\n",
    "        self.conv2 = self_BinarizeConv2D(64, 64, 3, 1, 1, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn2 = self_Batchnorm2d(64, bda_flag=bda_flag)\n",
    "        self.htanh2 = self_Hardtanh()\n",
    "        self.maxpooling2 = self_MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = self_BinarizeConv2D(64, 128, 3, 1, 1, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn3 = self_Batchnorm2d(128, bda_flag=bda_flag)\n",
    "        self.htanh3 = self_Hardtanh()\n",
    "\n",
    "        self.conv4 = self_BinarizeConv2D(128, 128, 3, 1, 1, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn4 = self_Batchnorm2d(128, bda_flag=bda_flag)\n",
    "        self.htanh4 = self_Hardtanh()\n",
    "        self.maxpooling4 = self_MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = self_BinarizeConv2D(128, 256, 3, 1, 1, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn5 = self_Batchnorm2d(256, bda_flag=bda_flag)\n",
    "        self.htanh5 = self_Hardtanh()\n",
    "\n",
    "        self.conv6 = self_BinarizeConv2D(256, 256, 3, 1, 1, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn6 = self_Batchnorm2d(256, bda_flag=bda_flag)\n",
    "        self.htanh6 = self_Hardtanh()\n",
    "\n",
    "        self.conv7 = self_BinarizeConv2D(256, 512, 3, 1, 1, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn7 = self_Batchnorm2d(512, bda_flag=bda_flag)\n",
    "        self.htanh7 = self_Hardtanh()\n",
    "        self.maxpooling7 = self_MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc14 = self_BinarizeLinear(4*4*512, 1024, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn14 = self_Batchnorm1d(1024, bda_flag=bda_flag)\n",
    "        self.htanh14 = self_Hardtanh()\n",
    "\n",
    "        self.fc15 = self_BinarizeLinear(1024, 1024, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.bn15 = self_Batchnorm1d(1024, bda_flag=bda_flag)\n",
    "        self.htanh15 = self_Hardtanh()\n",
    "\n",
    "        self.fc16 = self_BinarizeLinear(1024, 10, num_dir=num_dir, ba_flag=True, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "        self.CrossEntropy = self_Softmax_CrossEntropy()\n",
    "    \n",
    "    def train(self):\n",
    "        self.is_train = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_train = False\n",
    "\n",
    "    def change_bits(self, num):\n",
    "        self.bits_storage = self.bits_storage*(2**num)\n",
    "        self.conv1.bits_storage = self.bits_storage\n",
    "        self.conv2.bits_storage = self.bits_storage\n",
    "        self.conv3.bits_storage = self.bits_storage\n",
    "        self.conv4.bits_storage = self.bits_storage\n",
    "        self.conv5.bits_storage = self.bits_storage\n",
    "        self.conv6.bits_storage = self.bits_storage\n",
    "        self.conv7.bits_storage = self.bits_storage\n",
    "\n",
    "        self.fc14.bits_storage = self.bits_storage\n",
    "        self.fc15.bits_storage = self.bits_storage\n",
    "        self.fc16.bits_storage = self.bits_storage\n",
    "    \n",
    "    def forward(self, input, labels):\n",
    "        if self.is_train:\n",
    "            self.train_forward(input, labels)\n",
    "        \n",
    "        else:\n",
    "            self.eval_forward(input)\n",
    "    \n",
    "    def train_forward(self, input, labels):\n",
    "        da = None\n",
    "        x, da = self.conv1.train_forward(input, da)\n",
    "        x, da = self.bn1.train_forward(x, da)\n",
    "        x, da = self.htanh1.train_forward(x, da)\n",
    "\n",
    "        x, da = self.conv2.train_forward(x, da)\n",
    "        x, da = self.bn2.train_forward(x, da)\n",
    "        x, da = self.htanh2.train_forward(x, da)\n",
    "        x, da = self.maxpooling2.train_forward(x, da)\n",
    "\n",
    "        x, da = self.conv3.train_forward(x, da)\n",
    "        x, da = self.bn3.train_forward(x, da)\n",
    "        x, da = self.htanh3.train_forward(x, da)\n",
    "\n",
    "        x, da = self.conv4.train_forward(x, da)\n",
    "        x, da = self.bn4.train_forward(x, da)\n",
    "        x, da = self.htanh4.train_forward(x, da)\n",
    "        x, da = self.maxpooling4.train_forward(x, da)\n",
    "\n",
    "        x, da = self.conv5.train_forward(x, da)\n",
    "        x, da = self.bn5.train_forward(x, da)\n",
    "        x, da = self.htanh5.train_forward(x, da)\n",
    "\n",
    "        x, da = self.conv6.train_forward(x, da)\n",
    "        x, da = self.bn6.train_forward(x, da)\n",
    "        x, da = self.htanh6.train_forward(x, da)\n",
    "\n",
    "        x, da = self.conv7.train_forward(x, da)\n",
    "        x, da = self.bn7.train_forward(x, da)\n",
    "        x, da = self.htanh7.train_forward(x, da)\n",
    "        x, da = self.maxpooling7.train_forward(x, da)\n",
    "\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        da = da.reshape(da.size(0), da.size(1), -1)\n",
    "\n",
    "        x, da = self.fc14.train_forward(x, da)\n",
    "        x, da = self.bn14.train_forward(x, da)\n",
    "        x, da = self.htanh14.train_forward(x, da)\n",
    "\n",
    "        x, da = self.fc15.train_forward(x, da)\n",
    "        x, da = self.bn15.train_forward(x, da)\n",
    "        x, da = self.htanh15.train_forward(x, da)\n",
    "\n",
    "        x, da = self.fc16.train_forward(x, da)\n",
    "\n",
    "        self.output = x\n",
    "        loss, da = self.CrossEntropy.train_forward(x, labels, da)\n",
    "        self.loss, self.da = loss, da\n",
    "\n",
    "    def eval_forward(self, input):\n",
    "        x = self.conv1.eval_forward(input)\n",
    "        x = self.bn1.eval_forward(x)\n",
    "        x = self.htanh1.eval_forward(x)\n",
    "\n",
    "        x = self.conv2.eval_forward(x)\n",
    "        x = self.bn2.eval_forward(x)\n",
    "        x = self.htanh2.eval_forward(x)\n",
    "        x = self.maxpooling2.eval_forward(x)\n",
    "\n",
    "        x = self.conv3.eval_forward(x)\n",
    "        x = self.bn3.eval_forward(x)\n",
    "        x = self.htanh3.eval_forward(x)\n",
    "\n",
    "        x = self.conv4.eval_forward(x)\n",
    "        x = self.bn4.eval_forward(x)\n",
    "        x = self.htanh4.eval_forward(x)\n",
    "        x = self.maxpooling4.eval_forward(x)\n",
    "\n",
    "        x = self.conv5.eval_forward(x)\n",
    "        x = self.bn5.eval_forward(x)\n",
    "        x = self.htanh5.eval_forward(x)\n",
    "\n",
    "        x = self.conv6.eval_forward(x)\n",
    "        x = self.bn6.eval_forward(x)\n",
    "        x = self.htanh6.eval_forward(x)\n",
    "\n",
    "        x = self.conv7.eval_forward(x)\n",
    "        x = self.bn7.eval_forward(x)\n",
    "        x = self.htanh7.eval_forward(x)\n",
    "        x = self.maxpooling7.eval_forward(x)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = self.fc14.eval_forward(x)\n",
    "        x = self.bn14.eval_forward(x)\n",
    "        x = self.htanh14.eval_forward(x)\n",
    "\n",
    "        x = self.fc15.eval_forward(x)\n",
    "        x = self.bn15.eval_forward(x)\n",
    "        x = self.htanh15.eval_forward(x)\n",
    "\n",
    "        x = self.fc16.eval_forward(x)\n",
    "\n",
    "        self.output = x\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.conv1.update(self.da, lr)\n",
    "        self.conv2.update(self.da, lr)\n",
    "        self.conv3.update(self.da, lr)\n",
    "        self.conv4.update(self.da, lr)\n",
    "        self.conv5.update(self.da, lr)\n",
    "        self.conv6.update(self.da, lr)\n",
    "        self.conv7.update(self.da, lr)\n",
    "\n",
    "        self.fc14.update(self.da, lr)\n",
    "        self.fc15.update(self.da, lr)\n",
    "        self.fc16.update(self.da, lr)\n",
    "\n",
    "        self.bn1.update(self.da, lr)\n",
    "        self.bn2.update(self.da, lr)\n",
    "        self.bn3.update(self.da, lr)\n",
    "        self.bn4.update(self.da, lr)\n",
    "        self.bn5.update(self.da, lr)\n",
    "        self.bn6.update(self.da, lr)\n",
    "        self.bn7.update(self.da, lr)\n",
    "        self.bn14.update(self.da, lr)\n",
    "        self.bn15.update(self.da, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_dl, valid_dl, test_dl, num_dir, bda_flag, sw_flag, bits_storage, max_epochs, lr):\n",
    "    model = self_BVGG(num_dir=num_dir, bda_flag=bda_flag, sw_flag=sw_flag, bits_storage=bits_storage)\n",
    "    itr = -1\n",
    "    stats = {'train-loss' : [], 'valid-acc' : []}\n",
    "\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(max_epochs):\n",
    "            for x, y in train_dl:\n",
    "                itr += 1\n",
    "                model.forward(x, y)\n",
    "                model.update(lr)\n",
    "                stats['train-loss'].append((itr, model.loss.item()))\n",
    "                if itr != 0 and itr % 500 == 0:\n",
    "                    model.change_bits(1)\n",
    "                if itr % 100 == 0:\n",
    "                    #print(model.output)\n",
    "                    valid_acc = get_acc(model, valid_dl)\n",
    "                    stats['valid-acc'].append((itr, valid_acc))\n",
    "                    s = f\"{epoch}:{itr} [train] loss:{model.loss.item():.3f}, [valid] acc:{valid_acc:.3f}\"\n",
    "                    print(s)\n",
    "    \n",
    "    test_acc = get_acc(model, test_dl)\n",
    "    print(f\"[test] acc:{test_acc:.3f}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 128\n",
    "valid_batch = 1024\n",
    "\n",
    "num_dir = 20\n",
    "bda_flag = False\n",
    "sw_flag = True\n",
    "bits_storage = 8\n",
    "\n",
    "max_epochs = 20\n",
    "lr = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0:0 [train] loss:51.292, [valid] acc:0.106\n",
      "0:100 [train] loss:49.285, [valid] acc:0.108\n",
      "0:200 [train] loss:55.468, [valid] acc:0.107\n",
      "0:300 [train] loss:53.122, [valid] acc:0.106\n",
      "1:400 [train] loss:55.038, [valid] acc:0.107\n",
      "1:500 [train] loss:54.910, [valid] acc:0.099\n",
      "1:600 [train] loss:50.841, [valid] acc:0.109\n",
      "1:700 [train] loss:54.805, [valid] acc:0.099\n",
      "2:800 [train] loss:51.041, [valid] acc:0.101\n",
      "2:900 [train] loss:56.170, [valid] acc:0.113\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = get_Cifar10_dl(batch_size_train=train_batch, batch_size_eval=valid_batch, device=DEVICE)\n",
    "stats = run_experiment(train_dl, valid_dl, test_dl, num_dir, bda_flag, sw_flag, bits_storage, max_epochs, lr)\n",
    "print_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('BNN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb840f806f29f410adf72128552a18fefb24267895bb11ac579fa6a12231d74f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
