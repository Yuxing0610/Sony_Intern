{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is another version of the Memory augmented optimizer, instead of store the estimate gradient, we can store the random key that generate the tangent vector to save the memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import FashionMNIST, MNIST, CIFAR10, SVHN\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vision_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "DEVICE = torch.device('cuda')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset,device=None):\n",
    "    final_X, final_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        final_X.append(x)\n",
    "        final_Y.append(y)\n",
    "    X = torch.stack(final_X)\n",
    "    Y = torch.tensor(final_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return torch.utils.data.TensorDataset(X, Y)\n",
    "\n",
    "\n",
    "def get_mnist_dl(batch_size_train=256, batch_size_eval=256, device=torch.device('cpu')):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = MNIST('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [55000,5000])\n",
    "    \n",
    "    data_test = MNIST('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Net(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes=10) -> None:\n",
    "    super().__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(28*28, 1024)\n",
    "    #self.Relu1 = nn.ReLU()\n",
    "    self.Relu1 = nn.Hardtanh()\n",
    "    self.fc2 = nn.Linear(1024, 1024)\n",
    "    #self.Relu2 = nn.ReLU()\n",
    "    self.Relu2 = nn.Hardtanh()\n",
    "    self.fc3 = nn.Linear(1024, 1024)\n",
    "    #self.Relu3 = nn.ReLU()\n",
    "    self.Relu3 = nn.Hardtanh()\n",
    "    self.fc4 = nn.Linear(1024, num_classes)\n",
    "    self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.flatten(x)\n",
    "    x = self.Relu1(self.fc1(x))\n",
    "    x.data = x.data.sign()\n",
    "    x = self.Relu2(self.fc2(x))\n",
    "    x.data = x.data.sign()\n",
    "    x = self.Relu3(self.fc3(x))\n",
    "    x.data = x.data.sign()\n",
    "    x = self.fc4(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 26, kernel_size=5, stride=1, padding = 0)\n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(26, 52, kernel_size=3, stride=1, padding = 0)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(52, 10, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpooling3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc_1 = nn.Linear(5*5*10, 1024)\n",
    "        self.fc_2 = nn.Linear(1024, num_classes)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.maxpooling1(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpooling3(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1] for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, max(loss))\n",
    "  ax2.set_ylim(0.0, 1.05)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "  model.eval()\n",
    "  acc = []\n",
    "  for X, y in dl:\n",
    "    #acc.append((torch.sigmoid(model(X)) > 0.5) == y)\n",
    "    acc.append(torch.argmax(model(X), dim=1) == y)\n",
    "  acc = torch.cat(acc)\n",
    "  acc = torch.sum(acc)/len(acc)\n",
    "  model.train()\n",
    "  return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_decay(vector_buffer, decay_rate = 0.9):\n",
    "    new_buffer = {}\n",
    "    for key in vector_buffer:\n",
    "        new_buffer[key*decay_rate] = vector_buffer[key]\n",
    "    del vector_buffer\n",
    "    return new_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_aggregation(vector_buffer, cur_vector, dir_align):\n",
    "    res = {}\n",
    "    num = len(vector_buffer.keys())\n",
    "    if num == 0:\n",
    "        return cur_vector\n",
    "    \n",
    "    if dir_align:\n",
    "        dir_dict = torch.ones(len(vector_buffer), device = DEVICE)\n",
    "        \n",
    "        for j, key in enumerate(vector_buffer.keys()):\n",
    "            similarity = 0\n",
    "            for i in vector_buffer[key].keys():\n",
    "                similarity += torch.sum(vector_buffer[key][i] * cur_vector[i])\n",
    "\n",
    "            if similarity >= 0 :\n",
    "                dir_dict[j] = 1\n",
    "            else:\n",
    "                dir_dict[j] = -1\n",
    "\n",
    "\n",
    "        for j, key in enumerate(vector_buffer.keys()):\n",
    "            for i in vector_buffer[key].keys():\n",
    "                if i not in res.keys():\n",
    "                    res[i] = dir_dict[j]*vector_buffer[key][i]\n",
    "                else:\n",
    "                    res[i] += (dir_dict[j]*vector_buffer[key][i])\n",
    "\n",
    "\n",
    "    else:\n",
    "        for j, key in enumerate(vector_buffer.keys()):\n",
    "            for i in vector_buffer[key].keys():\n",
    "                if i not in res.keys():\n",
    "                    res[i] = vector_buffer[key][i]\n",
    "                else:\n",
    "                    res[i] += 1*vector_buffer[key][i]\n",
    "                    #print(vector_buffer[key][i] - 1*vector_buffer[key][i])\n",
    "    \n",
    "    for i in cur_vector.keys():\n",
    "        res[i] += cur_vector[i]\n",
    "        res[i] /= (num + 1)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_aggregation(vector_buffer, cur_vector, dir_align):\n",
    "    res = {}\n",
    "    num = len(vector_buffer.keys())\n",
    "    if num == 0:\n",
    "        return cur_vector\n",
    "        \n",
    "    if dir_align:\n",
    "        dir = torch.zeros(len(vector_buffer), device = DEVICE)\n",
    "        for j, key in enumerate(vector_buffer.keys()):\n",
    "            similarity = 0\n",
    "            for i in vector_buffer[key].keys():\n",
    "                similarity += torch.sum(vector_buffer[key][i] * cur_vector[i])\n",
    "            if similarity >= 0 :\n",
    "                dir[j] = 1\n",
    "            else:\n",
    "                dir[j] = -1\n",
    "\n",
    "        for j, key in enumerate(vector_buffer.keys()):\n",
    "            for i in vector_buffer[key].keys():\n",
    "                if i not in res.keys():\n",
    "                    res[i] = dir[j]*vector_buffer[key][i]\n",
    "                else:\n",
    "                    res[i] += (dir[j]*vector_buffer[key][i])\n",
    "    \n",
    "    else:\n",
    "        for key in vector_buffer.keys():\n",
    "            for i in vector_buffer[key].keys():\n",
    "                if i not in res.keys():\n",
    "                    res[i] = vector_buffer[key][i]\n",
    "                else:\n",
    "                    res[i] += vector_buffer[key][i]\n",
    "    \n",
    "    for i in cur_vector.keys():\n",
    "        '''\n",
    "        res[i] += cur_vector[i]\n",
    "        res[i] /= (num + 1)\n",
    "        '''\n",
    "        res[i] /= num\n",
    "        res[i] += cur_vector[i]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, opt, criterion, train_dl, valid_dl, test_dl, max_epochs=20, use_forward_grad=False, num_forward_grad=1, use_memory_augmented=False, buffer_capacity=5, decay_rate = 0.9, aggregate_method = \"mean\", dir_align = False):\n",
    "    itr = -1\n",
    "    stats = {'train-loss' : [], 'valid-acc' : []}\n",
    "    if use_memory_augmented:\n",
    "        vector_buffer = {}\n",
    "    random_dir = {}\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        random_dir[i] = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        for x, y in train_dl:\n",
    "            itr += 1\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            \n",
    "            if use_memory_augmented:\n",
    "                norm = 0\n",
    "            \n",
    "            if use_forward_grad:\n",
    "                with torch.no_grad():\n",
    "                    da = torch.zeros((num_forward_grad, 1), device = DEVICE)\n",
    "\n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        g = p.grad.view(-1)\n",
    "                        v = torch.randn(num_forward_grad, len(g), device = DEVICE)\n",
    "                        random_dir[i] = v\n",
    "                        da = da + (v @ g).view(num_forward_grad, 1)\n",
    "                    \n",
    "                    if use_memory_augmented:\n",
    "                        for i, p in enumerate(model.parameters()):\n",
    "                            g = (da * random_dir[i]).mean(dim = 0)\n",
    "                            norm += torch.norm(g)**2\n",
    "                        aggregate_vector = {} \n",
    "                        if aggregate_method == \"mean\":\n",
    "                            estimated_dir = mean_aggregation(vector_buffer, random_dir, dir_align)\n",
    "                        elif aggregate_method == \"sum\":\n",
    "                            estimated_dir = sum_aggregation(vector_buffer, random_dir, dir_align)\n",
    "                        vector_buffer = buffer_decay(vector_buffer, decay_rate)\n",
    "                        if len(vector_buffer.keys()) < buffer_capacity:\n",
    "                            vector_buffer[norm] = random_dir\n",
    "                        else:\n",
    "                            min_norm = min(vector_buffer.keys())\n",
    "                            if min_norm <= norm:\n",
    "                                del vector_buffer[min_norm]\n",
    "                                vector_buffer[norm] = random_dir\n",
    "\n",
    "                    else:\n",
    "                        estimated_dir = random_dir\n",
    "                    \n",
    "                    vector_norm = 0\n",
    "                    dim = 0\n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        vector_norm += torch.norm(estimated_dir[i])**2\n",
    "                        dim += len(estimated_dir[i][0])\n",
    "\n",
    "                    vector_norm = vector_norm**0.5\n",
    "\n",
    "                    da = torch.zeros((num_forward_grad, 1), device = DEVICE)\n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        g = p.grad.view(-1)\n",
    "                        v = estimated_dir[i]#/vector_norm\n",
    "                        da = da + (v @ g).view(num_forward_grad, 1)\n",
    "\n",
    "                    for i, p in enumerate(model.parameters()):\n",
    "                        #g = (da * (estimated_dir[i]/vector_norm)).mean(dim = 0)\n",
    "                        #g = g*dim\n",
    "                        g = (da * (estimated_dir[i])).mean(dim = 0)\n",
    "                        p.grad = g.view(p.grad.shape)\n",
    "            opt.step()\n",
    "\n",
    "            stats['train-loss'].append((itr, loss.item()))\n",
    "\n",
    "            if itr % 100 == 0:\n",
    "                valid_acc = get_acc(model, valid_dl)\n",
    "                stats['valid-acc'].append((itr, valid_acc))\n",
    "                s = f\"{epoch}:{itr} [train] loss:{loss.item():.3f}, [valid] acc:{valid_acc:.3f}\"\n",
    "                print(s)\n",
    "    \n",
    "    test_acc = get_acc(model, test_dl)\n",
    "    print(f\"[test] acc:{test_acc:.3f}\")\n",
    "\n",
    "    return stats\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Net().to(DEVICE)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=5e-4)\n",
    "max_epochs = 100\n",
    "criterian = nn.CrossEntropyLoss()\n",
    "use_forward_grad = True\n",
    "num_forward_grad = 1\n",
    "use_memory_augmented = True\n",
    "buffer_capacity = 5\n",
    "decay_rate = 0.9\n",
    "aggregate_method = 'mean'\n",
    "dir_align = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 [train] loss:2.380, [valid] acc:0.127\n",
      "0:100 [train] loss:nan, [valid] acc:0.097\n",
      "0:200 [train] loss:nan, [valid] acc:0.097\n",
      "1:300 [train] loss:nan, [valid] acc:0.097\n",
      "1:400 [train] loss:nan, [valid] acc:0.097\n",
      "2:500 [train] loss:nan, [valid] acc:0.097\n",
      "2:600 [train] loss:nan, [valid] acc:0.097\n",
      "3:700 [train] loss:nan, [valid] acc:0.097\n",
      "3:800 [train] loss:nan, [valid] acc:0.097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnormal(mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(g)), \u001b[39m0.\u001b[39m), std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(g)), \u001b[39m0.1\u001b[39m))\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     p\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(p\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m stats \u001b[39m=\u001b[39m run_experiment(model, opt, criterian, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_forward_grad, use_memory_augmented, buffer_capacity, decay_rate, aggregate_method, dir_align)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m print_stats(stats)\n",
      "\u001b[1;32m/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb Cell 12\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, opt, criterion, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_forward_grad, use_memory_augmented, buffer_capacity, decay_rate, aggregate_method, dir_align)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(num_forward_grad, \u001b[39mlen\u001b[39m(g), device \u001b[39m=\u001b[39m DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     random_dir[i] \u001b[39m=\u001b[39m v\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     da \u001b[39m=\u001b[39m da \u001b[39m+\u001b[39;49m (v \u001b[39m@\u001b[39;49m g)\u001b[39m.\u001b[39;49mview(num_forward_grad, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m use_memory_augmented:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Memory_Augmented_Optimizer_V2.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, p \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mparameters()):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = get_mnist_dl(device=DEVICE)\n",
    "\n",
    "for p in model.parameters():\n",
    "    g = p.view(-1)\n",
    "    v = torch.normal(mean = torch.full((1, len(g)), 0.), std = torch.full((1, len(g)), 0.1)).to(DEVICE)\n",
    "    p.data = v.view(p.shape)\n",
    "\n",
    "stats = run_experiment(model, opt, criterian, train_dl, valid_dl, test_dl, max_epochs, use_forward_grad, num_forward_grad, use_memory_augmented, buffer_capacity, decay_rate, aggregate_method, dir_align)\n",
    "print_stats(stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('BNN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb840f806f29f410adf72128552a18fefb24267895bb11ac579fa6a12231d74f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
