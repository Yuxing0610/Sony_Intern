{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import FashionMNIST, MNIST, CIFAR10, SVHN\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vision_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_device(dataset,device=None):\n",
    "    final_X, final_Y = [], []\n",
    "    for x, y in dataset:\n",
    "        final_X.append(x)\n",
    "        final_Y.append(y)\n",
    "    X = torch.stack(final_X)\n",
    "    Y = torch.tensor(final_Y)\n",
    "    if device is not None:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "    return torch.utils.data.TensorDataset(X, Y)\n",
    "\n",
    "\n",
    "def get_mnist_dl(batch_size_train=256, batch_size_eval=1024, device=torch.device('cuda')):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    data_train = MNIST('./datasets', train=True, download=True, transform=transform)\n",
    "    data_train = switch_to_device(data_train, device=device)\n",
    "    data_train, data_valid = torch.utils.data.random_split(data_train, [55000,5000])\n",
    "    \n",
    "    data_test = MNIST('./datasets', train=False, download=True, transform=transform)\n",
    "    data_test = switch_to_device(data_test, device=device)\n",
    "    \n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size_train, shuffle=True)\n",
    "    valid_dl = DataLoader(data_valid, batch_size=batch_size_eval, shuffle=False)\n",
    "    test_dl = DataLoader(data_test, batch_size=batch_size_eval, shuffle=False)\n",
    "    \n",
    "    return train_dl, valid_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Net(nn.Module):\n",
    "\n",
    "  def __init__(self, num_classes=10) -> None:\n",
    "    super().__init__()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(28*28, 1024)\n",
    "    self.Relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(1024, 1024)\n",
    "    self.Relu2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(1024, 1024)\n",
    "    self.Relu3 = nn.ReLU()\n",
    "    self.fc4 = nn.Linear(1024, num_classes)\n",
    "    #self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.flatten(x)\n",
    "    x = self.Relu1(self.fc1(x))\n",
    "    x = self.Relu2(self.fc2(x))\n",
    "    x = self.Relu3(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 26, kernel_size=5, stride=1, padding = 0)\n",
    "        self.maxpooling1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(26, 52, kernel_size=3, stride=1, padding = 0)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(52, 10, kernel_size=1, stride=1, padding=0)\n",
    "        self.maxpooling3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc_1 = nn.Linear(5*5*10, 1000)\n",
    "        self.fc_2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.maxpooling1(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpooling3(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = self.fc_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(stats):\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,3), dpi=110)\n",
    "  ax1.grid()\n",
    "  ax2.grid()\n",
    "\n",
    "  ax1.set_title(\"ERM loss\")\n",
    "  ax2.set_title(\"Valid Acc\")\n",
    "  \n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  ax2.set_xlabel(\"iterations\")\n",
    "\n",
    "  itrs = [x[0] for x in stats['train-loss']]\n",
    "  loss = [x[1] for x in stats['train-loss']]\n",
    "  ax1.plot(itrs, loss)\n",
    "\n",
    "  itrs = [x[0] for x in stats['valid-acc']]\n",
    "  acc = [x[1] for x in stats['valid-acc']]\n",
    "  ax2.plot(itrs, acc)\n",
    "\n",
    "  ax1.set_ylim(0.0, max(loss))\n",
    "  ax2.set_ylim(0.0, 1.05)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_acc(model, dl):\n",
    "  model.eval()\n",
    "  acc = []\n",
    "  for X, y in dl:\n",
    "    #acc.append((torch.sigmoid(model(X)) > 0.5) == y)\n",
    "    acc.append(torch.argmax(model(X), dim=1) == y)\n",
    "  acc = torch.cat(acc)\n",
    "  acc = torch.sum(acc)/len(acc)\n",
    "  model.train()\n",
    "  return acc.item()\n",
    "\n",
    "\n",
    "def run_experiment(model, opt, train_dl, valid_dl, test_dl, use_forward_grad=False, num_forward_grad=1, max_epochs=20):\n",
    "\n",
    "  itr = -1\n",
    "  stats = {'train-loss': [], 'valid-acc':[]}\n",
    "  time_list = []\n",
    "\n",
    "  for epoch in range(max_epochs):\n",
    "    for x, y in train_dl:\n",
    "        itr += 1\n",
    "        layer_inputs = {}\n",
    "        def hook(mod, input):\n",
    "            layer_inputs[mod] = input[0]\n",
    "        for module in model.modules():\n",
    "            module.register_forward_pre_hook(hook)\n",
    "        opt.zero_grad()\n",
    "        #start_2 = time.clock()\n",
    "        #print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        loss = F.cross_entropy(model(x), y)\n",
    "        #print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        loss.backward()\n",
    "        #print(torch.cuda.memory_allocated()/1024/1024)\n",
    "        \n",
    "        if use_forward_grad:\n",
    "          with torch.no_grad():\n",
    "            v_list = []\n",
    "            da = torch.zeros(num_forward_grad, 1).to(DEVICE)\n",
    "            for p in model.parameters():\n",
    "              g = p.grad.view(-1)\n",
    "              v = torch.randn(num_forward_grad, len(g), device=DEVICE)\n",
    "              v = v/v.norm()\n",
    "              da  = da + (v @ g).view(num_forward_grad,1)\n",
    "              v_list.append(v)\n",
    "              '''\n",
    "              g = ((v @ g).view(num_forward_grad,1) * v).mean(dim=0)\n",
    "              g = g*len(p.grad.view(-1))\n",
    "              p.grad = g.view(p.grad.shape)\n",
    "              '''\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "              g = (da * v_list[i]).mean(dim = 0)\n",
    "              g = g*len(p.grad.view(-1))\n",
    "              p.grad = g.view(p.grad.shape)\n",
    "        '''\n",
    "        if use_forward_grad:\n",
    "          with torch.no_grad():\n",
    "            #v_list = []\n",
    "            da = torch.zeros(num_forward_grad, 1).to(DEVICE)\n",
    "            for p in model.parameters():\n",
    "              g = p.grad.view(-1)\n",
    "              v = torch.randn(num_forward_grad, len(g), device=DEVICE)\n",
    "              source = torch.randn(len(g), device=DEVICE)\n",
    "              gap = math.floor(len(g)/num_forward_grad)\n",
    "              for i in range(num_forward_grad):\n",
    "                if i<num_forward_grad-1:\n",
    "                  v[i] = F.pad(source[i*gap:(i+1)*gap], pad = (i*gap, len(g)-(i+1)*gap))\n",
    "                else:\n",
    "                  v[i] = F.pad(source[i*gap:], pad = (i*gap, 0))\n",
    "              da  = da + (v @ g).view(num_forward_grad,1)\n",
    "              #v_list.append(v)\n",
    "              g = ((v @ g).view(num_forward_grad,1) * v).mean(dim=0)\n",
    "              p.grad = g.view(p.grad.shape)\n",
    " \n",
    "        \n",
    "        if use_forward_grad:\n",
    "          with torch.no_grad():\n",
    "            for name, module in model.named_modules():\n",
    "              if isinstance(module, torch.nn.Linear):\n",
    "                grad_w = module.weight.grad\n",
    "                approx = using_inputs_project(grad_w, layer_inputs[module])\n",
    "                module.weight.grad = approx\n",
    "                grad_b = module.bias.grad.view(-1)\n",
    "                v = torch.randn(num_forward_grad, len(grad_b), device=DEVICE)\n",
    "                grad_b = ((v @ grad_b).view(num_forward_grad,1) * v).mean(dim=0)\n",
    "                module.bias.grad = grad_b.view(module.bias.grad.shape)\n",
    "        '''\n",
    "        \n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25, norm_type=2)\n",
    "        opt.step()\n",
    "        #print(torch.cuda.memory_allocated()/1024/1024)\n",
    "\n",
    "        stats['train-loss'].append((itr, loss.item()))\n",
    "\n",
    "        if itr % 20 == 0:\n",
    "          valid_acc = get_acc(model, valid_dl)\n",
    "          stats['valid-acc'].append((itr, valid_acc))\n",
    "          s = f\"{epoch}:{itr} [train] loss:{loss.item():.3f}, [valid] acc:{valid_acc:.3f}\"\n",
    "          print(s)\n",
    "          time_list = []\n",
    "\n",
    "  test_acc = get_acc(model, test_dl)\n",
    "  print(f\"[test] acc:{test_acc:.3f}\")\n",
    "  return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_inputs_project(grad, input):\n",
    "    noise = torch.randn(grad.shape[0], device=DEVICE)\n",
    "    '''\n",
    "    inpt = torch.mean(input, dim = 0).view(-1)[None, :]\n",
    "\n",
    "    entry_from_batch = torch.randint(low=0, high=len(input), size=[])\n",
    "    inpt = input[entry_from_batch].view(-1)[None, :]\n",
    "    inpt /= inpt.norm()\n",
    "    '''\n",
    "    q,r = torch.qr(input.T)\n",
    "    entry_from_batch = torch.randint(low=0, high=len(input), size=[])\n",
    "    inpt = q.T[entry_from_batch].view(-1)[None, :]\n",
    "    expanded_noise = noise[:, None] * inpt\n",
    "    return expanded_noise * torch.sum(expanded_noise * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 [train] loss:14.662, [valid] acc:0.099\n",
      "0:20 [train] loss:nan, [valid] acc:0.099\n",
      "0:40 [train] loss:nan, [valid] acc:0.099\n",
      "0:60 [train] loss:nan, [valid] acc:0.099\n",
      "0:80 [train] loss:nan, [valid] acc:0.099\n",
      "0:100 [train] loss:nan, [valid] acc:0.099\n",
      "0:120 [train] loss:nan, [valid] acc:0.099\n",
      "0:140 [train] loss:nan, [valid] acc:0.099\n",
      "0:160 [train] loss:nan, [valid] acc:0.099\n",
      "0:180 [train] loss:nan, [valid] acc:0.099\n",
      "0:200 [train] loss:nan, [valid] acc:0.099\n",
      "1:220 [train] loss:nan, [valid] acc:0.099\n",
      "1:240 [train] loss:nan, [valid] acc:0.099\n",
      "1:260 [train] loss:nan, [valid] acc:0.099\n",
      "1:280 [train] loss:nan, [valid] acc:0.099\n",
      "1:300 [train] loss:nan, [valid] acc:0.099\n",
      "1:320 [train] loss:nan, [valid] acc:0.099\n",
      "1:340 [train] loss:nan, [valid] acc:0.099\n",
      "1:360 [train] loss:nan, [valid] acc:0.099\n",
      "1:380 [train] loss:nan, [valid] acc:0.099\n",
      "1:400 [train] loss:nan, [valid] acc:0.099\n",
      "1:420 [train] loss:nan, [valid] acc:0.099\n",
      "2:440 [train] loss:nan, [valid] acc:0.099\n",
      "2:460 [train] loss:nan, [valid] acc:0.099\n",
      "2:480 [train] loss:nan, [valid] acc:0.099\n",
      "2:500 [train] loss:nan, [valid] acc:0.099\n",
      "2:520 [train] loss:nan, [valid] acc:0.099\n",
      "2:540 [train] loss:nan, [valid] acc:0.099\n",
      "2:560 [train] loss:nan, [valid] acc:0.099\n",
      "2:580 [train] loss:nan, [valid] acc:0.099\n",
      "2:600 [train] loss:nan, [valid] acc:0.099\n",
      "2:620 [train] loss:nan, [valid] acc:0.099\n",
      "2:640 [train] loss:nan, [valid] acc:0.099\n",
      "3:660 [train] loss:nan, [valid] acc:0.099\n",
      "3:680 [train] loss:nan, [valid] acc:0.099\n",
      "3:700 [train] loss:nan, [valid] acc:0.099\n",
      "3:720 [train] loss:nan, [valid] acc:0.099\n",
      "3:740 [train] loss:nan, [valid] acc:0.099\n",
      "3:760 [train] loss:nan, [valid] acc:0.099\n",
      "3:780 [train] loss:nan, [valid] acc:0.099\n",
      "3:800 [train] loss:nan, [valid] acc:0.099\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     p\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(p\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m opt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m stats \u001b[39m=\u001b[39m run_experiment(model, opt, train_dl, valid_dl, test_dl, use_forward_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_forward_grad \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m print_stats(stats)\n",
      "\u001b[1;32m/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb Cell 7\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, opt, train_dl, valid_dl, test_dl, use_forward_grad, num_forward_grad, max_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m time_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m   \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_dl:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m       itr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226465737463307374726170703031222c2275736572223a22646579616f797578227d/home/deyaoyux/Gitlab/forward-mode-autodiff/Back_prop_simulation.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m       layer_inputs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataset.py:188\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/BNN/lib/python3.9/site-packages/torch/utils/data/dataset.py:188\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = get_mnist_dl(device=DEVICE)\n",
    "\n",
    "model = MLP_Net().to(DEVICE)\n",
    "\n",
    "for p in model.parameters():\n",
    "    g = p.view(-1)\n",
    "    v = torch.normal(mean = torch.full((1, len(g)), 0.), std = torch.full((1, len(g)), 0.1)).to(DEVICE)\n",
    "    p.data = v.view(p.shape)\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "stats = run_experiment(model, opt, train_dl, valid_dl, test_dl, use_forward_grad=True, num_forward_grad = 1, max_epochs=20)\n",
    "\n",
    "print_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('BNN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb840f806f29f410adf72128552a18fefb24267895bb11ac579fa6a12231d74f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
